{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1325d8f0",
   "metadata": {
    "cellId": "utpck4k43hbe4c2mpydqcw"
   },
   "source": [
    "# **Seminar 1 - Computer Vision**\n",
    "*Naumov Anton (Any0019)*\n",
    "\n",
    "*To contact me in telegram: @any0019*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff57a8cc",
   "metadata": {
    "cellId": "6wjwoiozvcmipo6l78fqse"
   },
   "source": [
    "## 1. Understanding convolutional (conv) layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42b7d41",
   "metadata": {
    "cellId": "7nteo2725ooclop5bdganh"
   },
   "source": [
    "### 1.1 Why do we need to understand this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3d85c",
   "metadata": {
    "cellId": "o6iv66cpnz8hph9uuwx2cf"
   },
   "source": [
    "1. **Model parameters** $\\longrightarrow$ How many params will model have if we'll use this conv layer, and what if we'll use 3 conv layers of one type, 10 of second type and 20 of third one? We can theoretically compute and analyze this things without any additional tools\n",
    "2. **$\\Delta H, \\Delta W, \\Delta C$** $\\longrightarrow$ What will happen to the image after going through this conv layer? Especially important when you want to stack **a lot** of them one after another or create atypical architecture\n",
    "3. **Receptive field** $\\longrightarrow$ How to interpret model results and find what went wrong, or how exactly model solved the case? How big of a patterns model can phisically capture?\n",
    "4. **It's not just magic!** $\\longrightarrow$ **It's cooler, it's mathematical magic!!!** And you can calculate and compute it in any way you want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016ab903",
   "metadata": {
    "cellId": "iwr5v8wa7bzllg8sctnmp"
   },
   "source": [
    "### 1.2 Conv layer params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8facd3a2",
   "metadata": {
    "cellId": "vpfghgwqumwal2e7rr2c"
   },
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "input: $H \\times W \\times C_{in}$\n",
    "\n",
    "1. Kernel\\_size $\\longrightarrow$ [$K_{h} \\times K_{w}$]\n",
    "2. Input and output channels $\\longrightarrow$ [$C_{in}$, $C_{out}$]\n",
    "3. Padding $\\longrightarrow$ how much zeroes (or other specified elements) should be added on each side of the image [$P$, default 0]\n",
    "4. Stride $\\longrightarrow$ distance between two adjacent positions of the kernel on the original image [$S$, default 1]\n",
    "5. Dilation $\\longrightarrow$ distance between each two adjacent elements inside the kernel [$D$, default 1]\n",
    "6. Bias $\\longrightarrow$ linear addition to each output channel [$B$, default True]\n",
    "\n",
    "output: $H' \\times W' \\times C_{out}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528f1841",
   "metadata": {
    "cellId": "gnco0pcu67qp9q7eow973p"
   },
   "source": [
    "<ul>\n",
    "<title><strong>Convolutional layer:</strong></title>\n",
    "<img src=https://miro.medium.com/max/535/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif alt=\"Convolutional layer\" width=\"30%\"/>\n",
    "\n",
    "<title><strong>Padding:</strong></title>\n",
    "<img src=https://miro.medium.com/max/395/1*1okwhewf5KCtIPaFib4XaA.gif alt=\"Padding\" width=\"30%\"/>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<title><strong>Stride:</strong></title>\n",
    "<img src=https://miro.medium.com/max/294/1*BMngs93_rm2_BpJFH2mS0Q.gif alt=\"Stride\" width=\"30%\"/>\n",
    "\n",
    "<title><strong>Dilation:</strong></title>\n",
    "<img src=https://miro.medium.com/max/395/0*7LCdr8W3gSQdnlSC.gif alt=\"Dilation\" width=\"30%\"/>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ace19b",
   "metadata": {
    "cellId": "7407l7o86ejdvzo67y7q"
   },
   "source": [
    "### 1.3 Model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a8b992",
   "metadata": {
    "cellId": "71an0rx9nj5gri792ujzup"
   },
   "source": [
    "Depends on $K_h, K_w, C_{in}, C_{out}, B$\n",
    "\n",
    "Actual kernel, moving across the image, has size $K_h \\times K_w \\times C_{in}$. Then, for each output channel we have one **different** kernel of this size and one bias value, if $B = \\text{true}$\n",
    "\n",
    "So, weights, corresponding to one convolutional layers are: $$((K_h * K_w * C_{in}) + B) * C_{out}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000099a6",
   "metadata": {
    "cellId": "y8wka7309gmgpvx4kwpsge"
   },
   "source": [
    "### 1.4 $\\Delta$ sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a96fcc",
   "metadata": {
    "cellId": "lv3tmaxh37kamewxygyk"
   },
   "source": [
    "Depends on everything, except bias\n",
    "\n",
    "Basically kernel of actual width $w'$ will substract $w'-1$ from original image (only $W - w' + 1$ possitions horizontally to put kernel)\n",
    "\n",
    "What is actual width of the kernel $w'$?  It's $w' = (K_w - 1) * D + 1$, because with dilation we're making kernel bigger, without adding any additional parameters, but instead making kernel sparse\n",
    "\n",
    "Padding adds $P$ extra elements on both sides of the image. So, if $P \\neq 0$ then $W = W + 2 P$\n",
    "\n",
    "With stride we ignore some of the available positions of the actual kernel to increase computational efficiency $W = \\Big\\lfloor \\frac{W - 1}{S} + 1 \\Big\\rfloor$\n",
    "\n",
    "Summarizing everything before: $$W' = \\Bigg\\lfloor \\frac{W + 2 P - D (K_w - 1) - 1}{S} + 1 \\Bigg\\rfloor$$\n",
    "\n",
    "Exactly the same applies to $H \\rightarrow H'$\n",
    "\n",
    "$C_{out}$ is set manullay and does not depend on anything else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56824c72",
   "metadata": {
    "cellId": "nff9q4r83z9chspqvmob"
   },
   "source": [
    "### 1.5 Receptive field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb9bde",
   "metadata": {
    "cellId": "73zt8fpglzp3uriuk6tva8"
   },
   "source": [
    "How much of a pattern model can possibly see? \n",
    "\n",
    "If convolutional model has only one layer, then the biggest possible pattern that model can capture is $K_h \\times K_w$\n",
    "\n",
    "If we'll use many small kernels consecutively, the last layer will be able to capture a patterns way bigger then it's kernel size. Receptive field - zone, from which a particular neuron actually takes information\n",
    "\n",
    "And... that's exactly the reason why we **STACK MORE LAYERS!!!** $\\longrightarrow$ it's just cheaper and more effective to have few small kernels, instead of one big"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d7238",
   "metadata": {
    "cellId": "jr5rxagloysmm0x4fizx1"
   },
   "source": [
    "**Simple example**:\n",
    "\n",
    "<img src=https://www.baeldung.com/wp-content/uploads/sites/4/2021/07/Capture1.png alt=\"Receptive field\" width=\"30%\" height=\"200\"/>\n",
    "\n",
    "So, how can we calculate it's size ourselves?\n",
    "\n",
    "For fixated neuron on layer $L$ it's receptive field on this layer equals 1. Then, recursively, each previous layer's receptive field size ($r_{l-1}$) for neuron on layer $L$ depends on kernel size ($k_l$) and stride ($s_{l}$) of conv layer applied after previous layer and this layer's receptive field size ($r_{l}$):\n",
    "$$r_{L} = 1; \\quad r_{l-1} = s_l r_l + (k_l - s_l)$$ \n",
    "\n",
    "$$\\text{Analytical formula}: \\quad r_0 = \\sum\\limits_{l=1}^{L} \\bigg( (k_l - 1) \\prod\\limits_{i=1}^{l-1} s_i \\bigg) + 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b0110",
   "metadata": {
    "cellId": "n5at9mh3so8ct1kju5b0r"
   },
   "source": [
    "### 1.6 Intermediate representations (will be more in part 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560e9fc",
   "metadata": {
    "cellId": "atz3mkk6es8g9731zzj14g"
   },
   "source": [
    "Sometimes it's very useful to see what model actually is looking for in the image, on which parts or patterns does it activates\n",
    "\n",
    "It helps with understanding of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a7c6c",
   "metadata": {
    "cellId": "ruosy9iwmyso0mzrqpkqi"
   },
   "source": [
    "## 2. Short intro to PyTorch (only required for this seminar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca2e11",
   "metadata": {
    "cellId": "bzieqglypoqps6ffog8xd"
   },
   "source": [
    "https://disk.yandex.ru/i/O3mQ76u43So3h9 (лекция Ежа)\n",
    "\n",
    "Приходим на следующей неделе на занятие по курсу Deep-CVG (там Ёж расскажет про PyTorch ещё больше и лучше)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f2fdc7",
   "metadata": {
    "cellId": "ik2brb8aj1mh8ojnuidol"
   },
   "source": [
    "1. Kinda similar to numpy in syntax, but instead of np.array uses torch.tensor\n",
    "2. Automatically and dynamically builds computational graph\n",
    "3. Built-in gradients and backpropagation\n",
    "4. A lot of NN-related stuff, like NN layers, optimizers, schedulers, dataloader, ...\n",
    "5. Have a really great set of documentations https://pytorch.org/docs/stable/index.html\n",
    "6. And even tutorials https://pytorch.org/tutorials/beginner/basics/intro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd7b10",
   "metadata": {
    "cellId": "jno5r4dp5ubg636mm3o8x6"
   },
   "source": [
    "### 2.1 Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "21b8bee1",
   "metadata": {
    "cellId": "3uin2fk6rmcka6xkc2f35q"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "import torch\n",
    "\n",
    "m = [[0, 1], [2, 3]]\n",
    "mt = torch.tensor(m)\n",
    "print(mt)\n",
    "\n",
    "m2 = np.array(m)\n",
    "m2t = torch.from_numpy(m2)\n",
    "print(m2t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "53bdad97",
   "metadata": {
    "cellId": "baplymw2uiaurnrl38crhq"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "mt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "id": "a547f3b5",
   "metadata": {
    "cellId": "h70u3wp3pud318cpj77xxc"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "print(mt.sum(dim=-1))\n",
    "print(mt.type(torch.DoubleTensor).sin())\n",
    "print(mt.type(torch.FloatTensor).log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "id": "90bfc899",
   "metadata": {
    "cellId": "pmzo84x57lqnrix9yvjqgn"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "mt.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "id": "eccef880",
   "metadata": {
    "cellId": "qfwpxryhk5cvrmj78wzlp"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "r = mt.detach().numpy()  # --> get numpy values from tensor vector\n",
    "# r = mt.item()  # --> get numpy value from tensor scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c76924",
   "metadata": {
    "cellId": "2b6psroh05byrfqknbxxoh"
   },
   "source": [
    "### 2.2 Datasets & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "d90e5e4b",
   "metadata": {
    "cellId": "yq8804q47573sxxllev2pw"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "\n",
    "# Datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "\n",
    "# On top of dataset for batching, shuffle, ...\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Augumentations for images\n",
    "from torchvision import transforms as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "e5adf75c",
   "metadata": {
    "cellId": "9u536txf5172j97g8ujaq6"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c72d7f",
   "metadata": {
    "cellId": "ng747nw9ci9fe1pty1y6sm"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "\n",
    "# # Usual use-case:\n",
    "\n",
    "# dataset_ = MyDataset()\n",
    "# dataloader_ = DataLoader(dataset_, batch_size=16, shuffle=True)\n",
    "# for ...:\n",
    "#     dataiter_ = iter(dataloader_)\n",
    "#     for (xs, ys, ...) in dataiter_:\n",
    "#         # xs --> batch of x as tensor of shape (batch_size, *x.shape)\n",
    "#         # do stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136e66d",
   "metadata": {
    "cellId": "29201llbppxu7e4oe180hi"
   },
   "source": [
    "### 2.3 Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "65a83b00",
   "metadata": {
    "cellId": "mc6lsynkb2x6gr3onyeka"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "\n",
    "# Neural networks layers\n",
    "from torch import nn\n",
    "\n",
    "# Some stuff as functions (not layers or anything else)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizers and lr_schedulers\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b42603",
   "metadata": {
    "cellId": "g1xif7y1ic988iget5ibki"
   },
   "source": [
    "**We will talk more about all of this further with examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80df89d",
   "metadata": {
    "cellId": "fmtpyp5d456m86eke5xmn"
   },
   "source": [
    "## 3. Let's have some fun with different data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4802a0",
   "metadata": {
    "cellId": "hxnwjebwnraimn4y736fmm"
   },
   "source": [
    "### 3.1 Downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a67afd",
   "metadata": {
    "cellId": "qjps89dgza9pmadi6sz5o"
   },
   "source": [
    "Downloading different datasets using https://pytorch.org/vision/stable/datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8353b4fc",
   "metadata": {
    "cellId": "wp8fioguatvdtgqyq7cb8"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "from torchvision import datasets\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from datetime import datetime \n",
    "\n",
    "download_dir = './datasets'\n",
    "\n",
    "all_datasets = [\n",
    "    'CIFAR10',\n",
    "    'CIFAR100',\n",
    "    'FashionMNIST',\n",
    "    'KMNIST',\n",
    "    'MNIST',\n",
    "    'STL10',\n",
    "    'SVHN',\n",
    "#     'CelebA'  # may give an error because of google drive, will write block to download manually if this will not work\n",
    "]\n",
    "\n",
    "val_data = dict()\n",
    "data = dict()\n",
    "download_time = dict()\n",
    "for i, dataset_name in enumerate(all_datasets):\n",
    "    clear_output(True)\n",
    "    start_time = datetime.now() \n",
    "    \n",
    "    print(f'{i+1}/{len(all_datasets)}: Downloading dataset {dataset_name}')\n",
    "    \n",
    "    download = not os.path.isdir(f'{download_dir}/{dataset_name}')\n",
    "    download_exec = f'data[dataset_name] = datasets.{dataset_name}(root=\\'{os.path.join(download_dir, dataset_name)}\\', download={download})'\n",
    "    if dataset_name not in {'CelebA', 'STL10', 'SVHN'}:\n",
    "        download_val_exec = f'val_data[dataset_name] = datasets.{dataset_name}(root=\\'{os.path.join(download_dir, dataset_name)}\\', download={download}, train=False)'\n",
    "    else:\n",
    "        download_val_exec = f'val_data[dataset_name] = datasets.{dataset_name}(root=\\'{os.path.join(download_dir, dataset_name)}\\', download={download}, split=\\'test\\')'\n",
    "    \n",
    "    # NEVER REPEAT THIS AT HOME\n",
    "    exec(download_exec)\n",
    "    exec(download_val_exec)\n",
    "    download_time[dataset_name] = datetime.now() - start_time\n",
    "\n",
    "clear_output(True)\n",
    "print('Download time:')\n",
    "for k, v in download_time.items():\n",
    "    print('{: <12} ~ {}'.format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d372c",
   "metadata": {
    "cellId": "3mwf2eixf82dkazexz2hrh"
   },
   "source": [
    "### 3.2 Let's look on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "cdc00d4a",
   "metadata": {
    "cellId": "qb2t4iwfe41dj5a4hjx20i"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "h = 1\n",
    "w = 5\n",
    "\n",
    "for dataset_name in data:    \n",
    "    fig, ax = plt.subplots(h, w, figsize=(20, 5 * h))\n",
    "    fig.suptitle(f'{dataset_name}:')\n",
    "    for i, el in enumerate(data[dataset_name]):\n",
    "        if i >= h * w:\n",
    "            break\n",
    "        plt.subplot(h, w, i + 1)\n",
    "        plt.imshow(el[0])\n",
    "        if isinstance(el[1], int):\n",
    "            try:\n",
    "                plt.title(data[dataset_name].classes[el[1]])\n",
    "            except:\n",
    "                plt.title(el[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868469b",
   "metadata": {
    "cellId": "7sw23gk3gsj8biceqood23"
   },
   "source": [
    "### 3.3 Making dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e9ee34b2",
   "metadata": {
    "cellId": "chtk63obo2ui4gqwc4x2g"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "# CODE ... IN THIS BLOCK\n",
    "#     original dataset has __init__, __len__, __getitem__\n",
    "class AddTransformsDataset:\n",
    "    # Class to add custom transforms to any dataset (except 'CelebA') before creating dataloader\n",
    "    def __init__(self, dataset, transforms=None):\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Getting classes from dataset\n",
    "        if hasattr(self.dataset, 'classes'):\n",
    "            self.clss = self.dataset.classes\n",
    "        else:\n",
    "            self.clss = [i for i in range(10)]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return ...\n",
    "    \n",
    "    def classes(self):\n",
    "        return self.clss\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        ...\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "eeb29ac6",
   "metadata": {
    "cellId": "kmtvczk1rstk7xxme3ce9"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "from torchvision import transforms as tr\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def my_transform():\n",
    "    return tr.Compose([\n",
    "        tr.ToTensor(),\n",
    "    ])\n",
    "\n",
    "def make_dataloaders(data, batch_size=16, transform_function=None, datasets_to_exclude=set(), val=False):\n",
    "    dataloaders = dict()\n",
    "    classes = dict()\n",
    "    if isinstance(datasets_to_exclude, (str, list, tuple)):\n",
    "        datasets_to_exclude = set(datasets_to_exclude)\n",
    "    \n",
    "    for dataset_name in data.keys() - {'CelebA'} - datasets_to_exclude:\n",
    "        ds = AddTransformsDataset(data[dataset_name], transform_function())\n",
    "        dataloaders[dataset_name] = DataLoader(\n",
    "            ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=(not val),\n",
    "            drop_last=(not val)\n",
    "        )\n",
    "        classes[dataset_name] = ds.classes()\n",
    "    return dataloaders, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "48c94178",
   "metadata": {
    "cellId": "beot7ncxi1ps4vg2i3mp"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "# dataloaders = make_dataloaders(..., 16, my_transform, set(all_datasets) - {'CIFAR10'}, val=...)  # if you want to make a dataloader for only CIFAR10\n",
    "dataloaders, classes = make_dataloaders(data, 16, my_transform)\n",
    "val_dataloaders, _ = make_dataloaders(val_data, 16, my_transform, val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "82ba51bc",
   "metadata": {
    "cellId": "8su4b51m5mnese961nnqo"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "dataloaders.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "48d99d15",
   "metadata": {
    "cellId": "1y59qr4hmtwfab0q8gz67o"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "for dataset_name in dataloaders:\n",
    "    imgs_batch, lbls_batch = next(iter(dataloaders[dataset_name]))\n",
    "    print(f'shapes for {dataset_name}:\\n--images_batch: {imgs_batch.shape},\\n--labels_batch: {lbls_batch.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d7693",
   "metadata": {
    "cellId": "wv1z9spexlo39ssufm0kkq"
   },
   "source": [
    "## 4. Building our first model and learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb2b47",
   "metadata": {
    "cellId": "kbjg2bptxdgzpk89rjejjc"
   },
   "source": [
    "### 4.1 Linear model (no conv layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f8b5b25a",
   "metadata": {
    "cellId": "5cnz87fx3qv0o9pivhkm5cj"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "# CODE ... IN THIS BLOCK\n",
    "#     nn.Linear(in_size, out_size)\n",
    "#     nn.Dropout(prob)\n",
    "#     nn.Flatten()\n",
    "#     nn.LogSoftmax(dim)\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class simple_net(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple linear network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, num_layers, hidden_sizes, activations, dropouts, output_size):\n",
    "        super(simple_net, self).__init__()\n",
    "        \n",
    "        if not isinstance(hidden_sizes, list):\n",
    "            hidden_sizes = [hidden_sizes] * num_layers\n",
    "        assert len(hidden_sizes) == num_layers, f'provide {num_layers} hidden_sizes or just one for all layers'\n",
    "        \n",
    "        if not isinstance(activations, list):\n",
    "            activations = [activations] * num_layers\n",
    "        assert len(activations) == num_layers, f'provide {num_layers} activation functions or just one for all layers'\n",
    "        \n",
    "        if not isinstance(dropouts, list):\n",
    "            dropouts = [dropouts] * num_layers\n",
    "        assert len(dropouts) == num_layers, f'provide {num_layers} dropout values or just one for all layers'\n",
    "        \n",
    "        flat = ('flat', nn.Flatten())\n",
    "        in_to_hid = ('in2hid', nn.Linear(input_size, hidden_sizes[0]))\n",
    "        \n",
    "        hid_ = [[\n",
    "            (f'act_{i+1}', ...),\n",
    "            (f'drop_{i+1}', ...),\n",
    "            (f'hid_{i+1}', ...)\n",
    "        ] for i in range(num_layers-1)]\n",
    "        hid = []\n",
    "        for el in hid_:\n",
    "            hid.extend(el)\n",
    "        \n",
    "        head = [\n",
    "            (f'act_{num_layers}', ...),\n",
    "            (f'drop_{num_layers}', ...),\n",
    "            ('hid2out', ...),\n",
    "            ('log-softmax', ...)\n",
    "        ]\n",
    "        \n",
    "        self.net = [flat, in_to_hid, *hid, *head]\n",
    "        self.net = nn.Sequential(OrderedDict(self.net))\n",
    "    \n",
    "    def forward(self, imgs):\n",
    "        return self.net(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "585ff3a1",
   "metadata": {
    "cellId": "w435m0gnnrd9p3ncf2foxf"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "# Example for CIFAR10\n",
    "model_lin = simple_net(\n",
    "    input_size = 3*32*32,\n",
    "    num_layers = 3,\n",
    "    hidden_sizes = [100, 100, 30],\n",
    "    activations = nn.Tanh(),\n",
    "    dropouts = 0.2,\n",
    "    output_size = 10\n",
    ")\n",
    "\n",
    "print('Model:', model_lin, sep='\\n')\n",
    "\n",
    "imgs_batch, lbls_batch = next(iter(dataloaders['CIFAR10']))\n",
    "\n",
    "print(f'\\nInput shape: {imgs_batch.shape}')\n",
    "out = model_lin(imgs_batch)\n",
    "print(f'Output shape: {out.shape}')\n",
    "\n",
    "print(f'\\nChecking that returned log-probabilities (all exp-sums must be close to 1)', out.exp().sum(-1).detach().numpy(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "f1bd628b",
   "metadata": {
    "cellId": "w205qi677boou072ow2y"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "# Counting how many parameters does our model have\n",
    "def model_num_params(model):\n",
    "    sum_params = 0\n",
    "    for param in model.named_parameters():\n",
    "        num_params = np.prod(param[1].shape)\n",
    "        print('{: <19} ~  {: <7} params'.format(param[0], num_params))\n",
    "        sum_params += num_params\n",
    "    print(f'\\nIn total: {sum_params} params')\n",
    "    return sum_params\n",
    "\n",
    "sum_params = model_num_params(model_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3bcd24",
   "metadata": {
    "cellId": "hylud9hnshb986836e4o8r"
   },
   "source": [
    "### 4.2 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0213038e",
   "metadata": {
    "cellId": "mz7o0bxfhzuktlualpcjl"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def create_model_and_optimizer(model_class, model_params, lr=1e-3, beta1=0.9, beta2=0.999, device=device):\n",
    "    model = model_class(**model_params)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr, [beta1, beta2])\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "85a51b11",
   "metadata": {
    "cellId": "3zd7edzrzbv27lr7zahxq8"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "model_params = {\n",
    "    'input_size': 3*32*32,\n",
    "    'num_layers': 3,\n",
    "    'hidden_sizes': [100, 100, 30],\n",
    "    'activations': nn.Tanh(),\n",
    "    'dropouts': 0.2,\n",
    "    'output_size': 10\n",
    "}\n",
    "\n",
    "model, optimizer = create_model_and_optimizer(\n",
    "    model_class = simple_net, \n",
    "    model_params = model_params,\n",
    "    lr = 1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e69388",
   "metadata": {
    "cellId": "gd6ajfl7b86gyb199oe3xh"
   },
   "source": [
    "### 4.3 Train/val one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "5ecd3f24",
   "metadata": {
    "cellId": "zfjcewfhznfdyky0b3qe9w"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "# CODE ... IN THIS BLOCK\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(model, optimizer, loader, criterion):\n",
    "    model.train()\n",
    "    losses_tr = []\n",
    "    for images, targets in tqdm(loader):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = ...\n",
    "        loss = ...\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses_tr.append(loss.item()) \n",
    "    \n",
    "    return model, optimizer, np.mean(losses_tr)\n",
    "\n",
    "def val(model, loader, criterion):\n",
    "    model.eval()\n",
    "    losses_val = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(loader):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            out = ...\n",
    "            loss = ...\n",
    "\n",
    "            losses_val.append(loss.item())\n",
    "    \n",
    "    return np.mean(losses_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d07e4",
   "metadata": {
    "cellId": "52qp85kftz7klx0qfdtn7g"
   },
   "source": [
    "### 4.4 Learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "a505d5ff",
   "metadata": {
    "cellId": "lxz71ntpitl9gebvittw"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "# CODE ... IN THIS BLOCK\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "\n",
    "def learning_loop(model, optimizer, train_loader, val_loader, criterion, scheduler=None, min_lr=None, epochs=10, val_every=1, draw_every=1, separate_show=False):\n",
    "    losses = {'train': [], 'val': []}\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f'#{epoch}/{epochs}:')\n",
    "        model, optimizer, loss = ...\n",
    "        losses['train'].append(loss)\n",
    "\n",
    "        if not (epoch % val_every):\n",
    "            loss = ...\n",
    "            losses['val'].append(loss)\n",
    "            if scheduler:\n",
    "                scheduler.step(loss)\n",
    "\n",
    "        if not (epoch % draw_every):\n",
    "            clear_output(True)\n",
    "            fig, ax = plt.subplots(1, 2 if separate_show else 1, figsize=(20, 10))\n",
    "            fig.suptitle(f'#{epoch}/{epochs}:')\n",
    "\n",
    "            if separate_show:\n",
    "                plt.subplot(121)\n",
    "                plt.title('loss on train')\n",
    "            plt.plot(losses['train'], 'r.-', label='train')\n",
    "            plt.legend()\n",
    "\n",
    "            if separate_show:\n",
    "                plt.subplot(122)\n",
    "                plt.title('loss on validation')\n",
    "            else:\n",
    "                plt.title('losses')\n",
    "            plt.plot(losses['val'], 'g.-', label='val')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.show()\n",
    "        \n",
    "        if min_lr and get_lr(optimizer) <= min_lr:\n",
    "            print(f'Learning process ended with early stop after epoch {epoch}')\n",
    "            break\n",
    "    \n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f1b41",
   "metadata": {
    "cellId": "hakmuzzceckndv63v8oau"
   },
   "source": [
    "### 4.5 Training linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "1455b6c0",
   "metadata": {
    "cellId": "fqlacet9a63cri80lwt2i"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "model_params = dict()\n",
    "model = dict()\n",
    "optimizer = dict()\n",
    "scheduler = dict()\n",
    "criterion = dict()\n",
    "losses = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "f30b3d4e",
   "metadata": {
    "cellId": "n8a9m9cksum766pbazkq3b"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "# CODE ... IN THIS BLOCK\n",
    "%%time\n",
    "\n",
    "chosen_dataset = ...\n",
    "model_type = 'simple'\n",
    "imgs_batch, lbls_batch = next(iter(dataloaders[chosen_dataset]))\n",
    "num_classes = len(classes[chosen_dataset])\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_params[model_type] = {\n",
    "    'input_size': np.prod(imgs_batch.shape[1:]),\n",
    "    'num_layers': ...,\n",
    "    'hidden_sizes': ...,\n",
    "    'activations': ...,\n",
    "    'dropouts': ...,\n",
    "    'output_size': num_classes\n",
    "}\n",
    "\n",
    "model[model_type], optimizer[model_type] = create_model_and_optimizer(\n",
    "    model_class = simple_net, \n",
    "    model_params = model_params[model_type],\n",
    "    lr = ...,\n",
    "    device = device,\n",
    ")\n",
    "\n",
    "scheduler[model_type] = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer[model_type], mode='min', factor=0.25, patience=4, threshold=0.001, verbose=True)\n",
    "\n",
    "criterion[model_type] = nn.NLLLoss()\n",
    "\n",
    "model[model_type], optimizer[model_type], losses[model_type] = learning_loop(\n",
    "    model = model[model_type],\n",
    "    optimizer = optimizer[model_type],\n",
    "    train_loader = dataloaders[chosen_dataset],\n",
    "    val_loader = val_dataloaders[chosen_dataset],\n",
    "    criterion = criterion[model_type],\n",
    "    scheduler = scheduler[model_type],\n",
    "    epochs = ...,\n",
    "    min_lr = ...\n",
    ")\n",
    "\n",
    "# if device != 'cpu':\n",
    "#     model[model_type] = model[model_type].to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3051539",
   "metadata": {
    "cellId": "3bqeyeqe1as7uib516d7j"
   },
   "source": [
    "### 4.6 Looking at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "ddbc9953",
   "metadata": {
    "cellId": "1gf1y1193mnu1nrt4b1t6l"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "def show_results(model, val_loader, class_labels):\n",
    "    with torch.no_grad():\n",
    "        imgs_batch, lbls_batch = next(iter(val_loader))\n",
    "        preds = model(imgs_batch.to(device)).exp()\n",
    "\n",
    "        plt.subplots(2, 8, figsize=(20, 12))\n",
    "        for i in range(imgs_batch.shape[0]):\n",
    "            plt.subplot(2, 8, i+1)\n",
    "            plt.imshow(imgs_batch[i,:,:,:].numpy().transpose(1, 2, 0))\n",
    "            title = f'original class: {class_labels[lbls_batch[i]]}\\n\\npredictions:'\n",
    "            for j, pred in enumerate(preds[i,:]):\n",
    "                title += '\\n{: <10} : {}'.format(class_labels[j], round(preds[i,j].item(), 4))\n",
    "            plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "show_results(model[model_type], val_dataloaders[chosen_dataset], classes[chosen_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "18f5b77b",
   "metadata": {
    "cellId": "sb64c9487scet2cuqe3h4w"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "def real_confusion_matrix(model, val_loader, class_labels, use_probs=False, normalize=True):\n",
    "    with torch.no_grad():\n",
    "        n_classes = len(class_labels)\n",
    "        conf_matrix = np.zeros((n_classes, n_classes))\n",
    "        for i, (images, labels) in enumerate(tqdm(val_loader)):\n",
    "            probs = model(images.to(device)).exp()\n",
    "            if use_probs:\n",
    "                for j in range(images.shape[0]):\n",
    "                    for c in range(n_classes):\n",
    "                        conf_matrix[labels[j].item(), c] += probs[j,c]\n",
    "            else:\n",
    "                _, pred_classes = torch.max(probs, 1)\n",
    "                for j in range(images.shape[0]):\n",
    "                    conf_matrix[labels[j].item(), pred_classes[j].item()] += 1.\n",
    "        \n",
    "        if normalize:\n",
    "            conf_matrix /= conf_matrix.sum(1)\n",
    "        \n",
    "        fig = plt.figure(figsize=(18, 10))\n",
    "        fig.suptitle(f'Confusion matrix (norm={normalize}, use_probs={use_probs})')\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(conf_matrix)\n",
    "        fig.colorbar(cax)\n",
    "        \n",
    "        ax.xaxis.set_major_formatter('')\n",
    "        secax = ax.secondary_xaxis('top')\n",
    "        secax.xaxis.set_ticks(list(range(len(class_labels))))\n",
    "        secax.xaxis.set_ticklabels(class_labels)\n",
    "        secax.set_xlabel('predicted class')\n",
    "        \n",
    "        ax.yaxis.set_ticks(list(range(len(class_labels))))\n",
    "        ax.yaxis.set_ticklabels(class_labels)\n",
    "        ax.set_ylabel('true class')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return conf_matrix\n",
    "\n",
    "pcm = real_confusion_matrix(model[model_type], val_dataloaders[chosen_dataset], classes[chosen_dataset], use_probs=True, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c0520",
   "metadata": {
    "cellId": "gxhgz61dyvjszczag4pj2"
   },
   "source": [
    "## 5. Trying convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cf52cc",
   "metadata": {
    "cellId": "zl5sxot5rn9phzszjrt0w"
   },
   "source": [
    "### 5.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "edad5abc",
   "metadata": {
    "cellId": "0kyknkhfdvjhm8tv4ftdrt"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "# CODE ... IN THIS BLOCK\n",
    "#     nn.Linear(in_size, out_size)\n",
    "#     nn.Dropout(prob)\n",
    "#     nn.Flatten()\n",
    "#     nn.LogSoftmax(dim)\n",
    "#     nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class conv_net(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple convolutional network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, num_layers, kernel_sizes, hidden_sizes, activations, dropouts, output_size):\n",
    "        super(conv_net, self).__init__()\n",
    "        \n",
    "        if not isinstance(kernel_sizes, list):\n",
    "            kernel_sizes = [kernel_sizes] * num_layers\n",
    "        assert len(kernel_sizes) == num_layers, f'provide {num_layers} kernel_sizes or just one for all layers'\n",
    "        \n",
    "        if not isinstance(hidden_sizes, list):\n",
    "            hidden_sizes = [hidden_sizes] * num_layers\n",
    "        assert len(hidden_sizes) == num_layers, f'provide {num_layers} hidden_sizes or just one for all layers'\n",
    "        \n",
    "        if not isinstance(activations, list):\n",
    "            activations = [activations] * num_layers\n",
    "        assert len(activations) == num_layers, f'provide {num_layers} activation functions or just one for all layers'\n",
    "        \n",
    "        if not isinstance(dropouts, list):\n",
    "            dropouts = [dropouts] * num_layers\n",
    "        assert len(dropouts) == num_layers, f'provide {num_layers} dropout values or just one for all layers'\n",
    "        \n",
    "        in_conv = ('conv_0', ...)\n",
    "        \n",
    "        conv_blocks_ = [[\n",
    "            (f'act_{i+1}', ...),\n",
    "            (f'drop_{i+1}', ...),\n",
    "            (f'conv_{i+1}', ...)\n",
    "        ] for i in range(num_layers-1)]\n",
    "        conv_blocks = []\n",
    "        for el in conv_blocks_:\n",
    "            conv_blocks.extend(el)\n",
    "        \n",
    "        head = [\n",
    "            (f'act_{num_layers}', ...),\n",
    "            (f'drop_{num_layers}', ...),\n",
    "            ('flat', ...),\n",
    "            ('hid2out', ...),\n",
    "            ('log-softmax', ...)\n",
    "        ]\n",
    "        \n",
    "        self.net = [in_conv, *conv_blocks, *head]\n",
    "        self.net = nn.Sequential(OrderedDict(self.net))\n",
    "    \n",
    "    def forward(self, imgs):\n",
    "        return self.net(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "20db10d0",
   "metadata": {
    "cellId": "ht3r4balwahdmmybbyty2m"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "# Example for CIFAR10\n",
    "model_conv = conv_net(\n",
    "    input_size = (3, 32, 32),\n",
    "    num_layers = 3,\n",
    "    kernel_sizes = 5,\n",
    "    hidden_sizes = [16, 16, 8],\n",
    "    activations = nn.Tanh(),\n",
    "    dropouts = 0.2,\n",
    "    output_size = 10\n",
    ")\n",
    "\n",
    "print('Model:', model_conv, sep='\\n')\n",
    "\n",
    "imgs_batch, lbls_batch = next(iter(dataloaders['CIFAR10']))\n",
    "\n",
    "print(f'\\nInput shape: {imgs_batch.shape}')\n",
    "out = model_conv(imgs_batch)\n",
    "print(f'Output shape: {out.shape}')\n",
    "\n",
    "print(f'\\nChecking that returned log-probabilities (all exp-sums must be close to 1)', out.exp().sum(-1).detach().numpy(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "e48a9782",
   "metadata": {
    "cellId": "czykoubd3m5tataw5v7ac"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "# Counting how many parameters does our model have\n",
    "sum_params = model_num_params(model_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2374496",
   "metadata": {
    "cellId": "uh3sd6iytlce3k9441vroi"
   },
   "source": [
    "### 5.2 Learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "155c0597",
   "metadata": {
    "cellId": "sev34iab74nshfp6q2abai"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "%%time\n",
    "\n",
    "chosen_dataset = ...\n",
    "model_type = 'conv_easy'\n",
    "imgs_batch, lbls_batch = next(iter(dataloaders[chosen_dataset]))\n",
    "num_classes = len(classes[chosen_dataset])\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_params[model_type] = {\n",
    "    'input_size': imgs_batch.shape[1:],\n",
    "    'num_layers': ...,\n",
    "    'kernel_sizes': ...,\n",
    "    'hidden_sizes': ...,\n",
    "    'activations': ...,\n",
    "    'dropouts': ...,\n",
    "    'output_size': num_classes\n",
    "}\n",
    "\n",
    "model[model_type], optimizer[model_type] = create_model_and_optimizer(\n",
    "    model_class = conv_net, \n",
    "    model_params = model_params[model_type],\n",
    "    lr = ...,\n",
    "    device = device,\n",
    ")\n",
    "\n",
    "scheduler[model_type] = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer[model_type], mode='min', factor=0.25, patience=4, threshold=0.001, verbose=True)\n",
    "\n",
    "criterion[model_type] = nn.NLLLoss()\n",
    "\n",
    "model[model_type], optimizer[model_type], losses[model_type] = learning_loop(\n",
    "    model = model[model_type],\n",
    "    optimizer = optimizer[model_type],\n",
    "    train_loader = dataloaders[chosen_dataset],\n",
    "    val_loader = val_dataloaders[chosen_dataset],\n",
    "    criterion = criterion[model_type],\n",
    "    scheduler = scheduler[model_type],\n",
    "    epochs = ...,\n",
    "    min_lr = ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "4f235bad",
   "metadata": {
    "cellId": "m9asbnoz8mhbbdsr45fh"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "show_results(model[model_type], val_dataloaders[chosen_dataset], classes[chosen_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "b370da0e",
   "metadata": {
    "cellId": "i703v4e2hioeg5931sxv9i"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "pcm = real_confusion_matrix(model[model_type], val_dataloaders[chosen_dataset], classes[chosen_dataset], use_probs=True, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a98d9a",
   "metadata": {
    "cellId": "0vkkkkuxk83oxmj0tuclw7l"
   },
   "source": [
    "## 6. Analyzing results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b7a8b4",
   "metadata": {
    "cellId": "bw4j3iw87re3rs3a6troso"
   },
   "source": [
    "Few good links\n",
    "\n",
    "https://distill.pub/2017/feature-visualization $\\longrightarrow$ feature visualization\n",
    "\n",
    "https://yosinski.com/deepvis $\\longrightarrow$ deep visualisation toolbox\n",
    "\n",
    "https://www.programmersought.com/article/23176813233 $\\longrightarrow$ visualize the features of CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45bcffc",
   "metadata": {
    "cellId": "d5lmuc2lo4ijwqb2e0lkif"
   },
   "source": [
    "### 6.1 What model looks like inside? Intermediate representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "e4ccae9c",
   "metadata": {
    "cellId": "3lb893fe0grusv76z6iyu"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "mt_lin = 'simple'\n",
    "mt_conv = 'conv_easy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "418704c4",
   "metadata": {
    "cellId": "elijunl0l9nkw2pibi1fl"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "model[mt_lin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "62c42621",
   "metadata": {
    "cellId": "ohwvil2xhun40pe6jrqd5h"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "from termcolor import colored\n",
    "\n",
    "print(colored('Per layer intermediate representation of activations for linear model', attrs=['bold']))\n",
    "\n",
    "h, w = 1, 5\n",
    "with_color = True\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = None\n",
    "    layer_num = 0\n",
    "    for name, param in list(model[mt_lin].named_parameters()):\n",
    "        pref = name.split('.')[-1]\n",
    "        np_data = np.copy(param.data.numpy().T)\n",
    "        if pref == 'weight':\n",
    "            layer_num += 1\n",
    "            if res is not None:\n",
    "                res = res @ np_data\n",
    "            else:\n",
    "                res = np_data\n",
    "\n",
    "            H = W = np.sqrt(res.shape[0] // 3).astype(np.int) if with_color else np.sqrt(res.shape[0]).astype(np.int)\n",
    "            internal = np.copy(res.reshape(3 if with_color else 1, H, W, res.shape[-1]))\n",
    "            internal -= np.min(internal, axis=(0, 1, 2))[None, None, None, :]\n",
    "            internal /= np.max(internal, axis=(0, 1, 2))[None, None, None, :]\n",
    "\n",
    "            fig, ax = plt.subplots(h, w, figsize=(4*w, 4*h))\n",
    "            fig.suptitle(f'Layer #{layer_num} ({name.split(\".\")[1]})')\n",
    "            for i in range(h*w):\n",
    "                plt.subplot(h, w, i+1)\n",
    "                img_ = internal[:,:,:,i]\n",
    "                plt.imshow(img_.transpose(1, 2, 0), aspect='auto')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "99d3da49",
   "metadata": {
    "cellId": "bux7uxhxu324zp318wd91"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "def kinda_internal_activations(lin_model, with_color=True):\n",
    "    with torch.no_grad():\n",
    "        res = None\n",
    "        for name, param in list(lin_model.named_parameters()):\n",
    "            pref = name.split('.')[-1]\n",
    "            np_data = np.copy(param.data.numpy().T)\n",
    "            if pref == 'weight':\n",
    "                if res is not None:\n",
    "                    res = res @ np_data\n",
    "                else:\n",
    "                    res = np_data\n",
    "\n",
    "        H = W = np.sqrt(res.shape[0] // 3).astype(np.int) if with_color else np.sqrt(res.shape[0]).astype(np.int)\n",
    "        res = res.reshape(3 if with_color else 1, H, W, res.shape[-1])\n",
    "\n",
    "        res -= np.min(res, axis=(0,1,2))\n",
    "        res /= np.max(res, axis=(0,1,2))\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "res = kinda_internal_activations(model[mt_lin], True)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(20, 8))\n",
    "fig.suptitle('Linear model activations')\n",
    "for i in range(res.shape[-1]):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.title(classes[chosen_dataset][i])\n",
    "    plt.imshow(res[:,:,:,i].transpose(1, 2, 0), aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee2f7c",
   "metadata": {
    "cellId": "atk5y3l35zinu106ob1og"
   },
   "source": [
    "### 6.2 Convolutional internal representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "d68b9c34",
   "metadata": {
    "cellId": "783l6x56r62zsng30bo5me"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "model[mt_conv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "20490d5c",
   "metadata": {
    "cellId": "yxra7j5zh2rxb7yos2u1k"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "for name, param in list(model[mt_conv].named_parameters()):\n",
    "    print(name, '\\n', param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "615a6a07",
   "metadata": {
    "cellId": "njuiyj1nrkes9qrzes0x9k"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "def conv_kernels(conv_model):\n",
    "    with torch.no_grad():\n",
    "        first = True\n",
    "        for name, param in list(conv_model.named_parameters()):\n",
    "            sp = name.split('.')\n",
    "            np_data = np.copy(param.data.numpy())\n",
    "            if sp[-1] == 'weight' and 'conv' in sp[1]:\n",
    "                if first:\n",
    "                    W = 8\n",
    "                    H = np.ceil(np_data.shape[0] / W).astype(np.int)\n",
    "                    fig, ax = plt.subplots(H, W, figsize=(20, 5))\n",
    "                    fig.suptitle(f'layer {sp[1]} kernels')\n",
    "                    for img_i in range(np_data.shape[0]):\n",
    "                        plt.subplot(H, W, img_i+1)\n",
    "                        plt.tick_params(which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "                        img_ = np_data[img_i].transpose(1, 2, 0)\n",
    "                        img_ -= np.min(img_)\n",
    "                        img_ /= np.max(img_)\n",
    "                        plt.imshow(img_)\n",
    "                    plt.show()\n",
    "                    first = False\n",
    "                else:\n",
    "                    H = np_data.shape[0]\n",
    "                    W = np_data.shape[1]\n",
    "                    fig, ax = plt.subplots(H, W, figsize=(20, H - 1))\n",
    "                    fig.suptitle(f'layer {sp[1]} kernels')\n",
    "                    plt.xlabel('input channels')\n",
    "                    plt.ylabel('output channels')\n",
    "                    \n",
    "                    for i in range(H):\n",
    "                        for j in range(W):\n",
    "                            plt.subplot(H, W, i*W + j + 1)\n",
    "                            plt.tick_params(which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "                            plt.imshow(np_data[i, j])\n",
    "                    plt.show()\n",
    "\n",
    "conv_kernels(model[mt_conv])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc88f70",
   "metadata": {
    "cellId": "t5b42v5jlm844rk8mr931m"
   },
   "source": [
    "### 6.3 t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "9ebdd2c5",
   "metadata": {
    "cellId": "xhagj641o8qbwtr9aft4t"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_tsne(model_, val_loader, pre_output=True, random_state=19):\n",
    "    if pre_output:\n",
    "        new_model = nn.Sequential(*list(model_.net.children())[:-2])\n",
    "    else:\n",
    "        new_model = nn.Sequential(*list(model_.net.children()))\n",
    "    for param in new_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    internal_vectors = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader):\n",
    "            images = images.to(device)\n",
    "            out = new_model(images)\n",
    "            \n",
    "            internal_vectors.append(out.detach().numpy())\n",
    "            all_targets.append(targets.numpy())\n",
    "    \n",
    "    internal_vectors = np.concatenate(internal_vectors, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    tsne_vecs = TSNE(\n",
    "        n_components=2,\n",
    "#         verbose=3,\n",
    "        init='pca',\n",
    "        random_state=random_state,\n",
    "        n_iter=2000,\n",
    "        n_jobs=-1\n",
    "    ).fit_transform(internal_vectors)\n",
    "    \n",
    "    return tsne_vecs, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "e3f5fe9d",
   "metadata": {
    "cellId": "b2qrofh5obb38pbz0mbd2l"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "%%time\n",
    "clear_output(True)\n",
    "print('starting convolutional t-sne')\n",
    "vecs_conv, tgts_conv = plot_tsne(model[mt_conv], val_dataloaders[chosen_dataset])\n",
    "print('starting linear t-sne')\n",
    "vecs_lin, tgts_lin = plot_tsne(model[mt_lin], val_dataloaders[chosen_dataset])\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "9e0ec9c8",
   "metadata": {
    "cellId": "56rp5s28ncvvopyof801j"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "alpha = 0.2\n",
    "s = 10\n",
    "\n",
    "n_cl = len(classes[chosen_dataset])\n",
    "colors = cm.rainbow(np.linspace(0, 1, n_cl))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "fig.suptitle('Pre output')\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('conv model')\n",
    "for g in np.unique(tgts_conv):\n",
    "    ix = np.where(tgts_conv == g)\n",
    "    plt.scatter(vecs_conv[ix,0], vecs_conv[ix,1], color=colors[g], label=classes[chosen_dataset][g], s=s, alpha=alpha)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('linear model')\n",
    "for g in np.unique(tgts_lin):\n",
    "    ix = np.where(tgts_lin == g)\n",
    "    plt.scatter(vecs_lin[ix,0], vecs_lin[ix,1], color=colors[g], label=classes[chosen_dataset][g], s=s, alpha=alpha)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "900bc3ff",
   "metadata": {
    "cellId": "72nuxwmdhvja2bkrg7zu9v"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "%%time\n",
    "clear_output(True)\n",
    "print('starting convolutional t-sne')\n",
    "vecs_conv_, tgts_conv_ = plot_tsne(model[mt_conv], val_dataloaders[chosen_dataset], False)\n",
    "print('starting linear t-sne')\n",
    "vecs_lin_, tgts_lin_ = plot_tsne(model[mt_lin], val_dataloaders[chosen_dataset], False)\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "654a7f5c",
   "metadata": {
    "cellId": "hb32jem7sf5q9v0h2qw3f"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "alpha = 0.2\n",
    "s = 10\n",
    "\n",
    "n_cl = len(classes[chosen_dataset])\n",
    "colors = cm.rainbow(np.linspace(0, 1, n_cl))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "fig.suptitle('Output')\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('conv model')\n",
    "for g in np.unique(tgts_conv_):\n",
    "    ix = np.where(tgts_conv_ == g)\n",
    "    plt.scatter(vecs_conv_[ix,0], vecs_conv_[ix,1], color=colors[g], label=classes[chosen_dataset][g], s=s, alpha=alpha)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('linear model')\n",
    "for g in np.unique(tgts_lin_):\n",
    "    ix = np.where(tgts_lin_ == g)\n",
    "    plt.scatter(vecs_lin_[ix,0], vecs_lin_[ix,1], color=colors[g], label=classes[chosen_dataset][g], s=s, alpha=alpha)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a32ad00",
   "metadata": {
    "cellId": "wqglsdzwjhdrlyret90q5"
   },
   "source": [
    "## 7. What else can we do to improve convolutional model's quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec3ea2",
   "metadata": {
    "cellId": "pvz18p5tuikm90yfle89yd"
   },
   "source": [
    "### 7.1 Image preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c2682",
   "metadata": {
    "cellId": "ia35t8jtmchqnusluc4cv9"
   },
   "source": [
    "Data preprocessings and augumentations are very important\n",
    "\n",
    "Always think about how this exact transformation will affect input image and would this help or not\n",
    "\n",
    "https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py $\\longrightarrow$ examples\n",
    "\n",
    "https://pytorch.org/vision/stable/transforms.html $\\longrightarrow$ pytorch transforms documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40218081",
   "metadata": {
    "cellId": "c33l8icpg5fx4vdpl5qdla"
   },
   "source": [
    "- **Normalization** $\\longrightarrow$ extremely important in terms of learning stability\n",
    "- **Resize** $\\longrightarrow$ can be used in a lot of various ways, but most oftenly used to make all the\n",
    "- **Crops**\n",
    "- **Flips**\n",
    "- **Blur**\n",
    "- **Turn**\n",
    "- **Grayscale**\n",
    "- **Brightness/colors/...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad98a0",
   "metadata": {
    "cellId": "cs2wi5wde98b5w29lvc5sd"
   },
   "source": [
    "### 7.2 Special tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fae63e",
   "metadata": {
    "cellId": "g9sp66k23zcmd5of65zar"
   },
   "source": [
    "- **Residual connections** $\\longrightarrow$ especially important for deep NN, helps avoiding vanishing gradient\n",
    "- **Layer normalization** $\\longrightarrow$ helps avoiding exploding or vanishing gradients, makes learning more stable\n",
    "- **Max/average poolings** $\\longrightarrow$ computationally cheap and easy way of reducing dimensionality of current image, while preserving data. Also is non-linear layer, works really well (emperically) with ReLU activations\n",
    "- **Global poolings** $\\longrightarrow$ can be used instead of flatten + linear head, in terms of convNN is's bettter, because logically it's more important if we found the pattern or not, instead of is's final position. Second important advantage of global pooling - input images can be not of the same size. Also all benefits of poolings are still applied here\n",
    "- **Adding random noise to the image** $\\longrightarrow$ regularization technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cbb778",
   "metadata": {
    "cellId": "5h8griagxcfemaojqekm2w"
   },
   "source": [
    "### 7.3 Finetuning & Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34076b3d",
   "metadata": {
    "cellId": "0m9vh8a1o96rlk1wle1vf"
   },
   "source": [
    "We can take some big and already trained model (e.g. resnet) and finetune it\n",
    "\n",
    "**Finetuning** $\\longrightarrow$ initializing model from model already trained on simmiliar task with (may be) additional head at the end, then training this model with lower learning rate on our data\n",
    "\n",
    "Or we can use transfer learning\n",
    "\n",
    "**Transfer learning** $\\longrightarrow$ removing head, freezing most of the parameters (may be except a few last layers) and changing the head to our needs. Then training this model (but in fact just the new head) on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "fd92ac7f",
   "metadata": {
    "cellId": "56q1cqotc0rizsgo1sfkjn"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "from torchvision import models\n",
    "\n",
    "ft_model = models.resnet18(pretrained=True, progress=True)\n",
    "\n",
    "ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "bd212564",
   "metadata": {
    "cellId": "ltllbqcstihnajhzxj59ai"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "\n",
    "# If you want transfer learning instead of finetuning:\n",
    "# -----vvv-----\n",
    "for name, param in ft_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "# -----^^^-----\n",
    "\n",
    "# here you set your own number of classes\n",
    "num_classes = 10\n",
    "\n",
    "# getting input size\n",
    "input_features = ft_model.fc.in_features\n",
    "\n",
    "# changing final classificator\n",
    "ft_model.fc = nn.Linear(input_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "1cb284b8",
   "metadata": {
    "cellId": "3t99suweg7hvuoepy8rzic"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "ft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60cd036",
   "metadata": {
    "cellId": "eugph9nbz5sievn13oyozi"
   },
   "source": [
    "## 8. Teaser of what else can be done with convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260e010",
   "metadata": {
    "cellId": "v3041l8dy2db1qjyh9abro"
   },
   "source": [
    "## 9. CelebA - you can try this out (individual work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fe48766b",
   "metadata": {
    "cellId": "pz0tv25opejov1todvcyk9"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "k_show = 5\n",
    "fig, ax = plt.subplots(1, k_show, figsize=(20, 7))\n",
    "for k, el in enumerate(data['CelebA']):\n",
    "    if k >= k_show:\n",
    "        break\n",
    "    plt.subplot(1, k_show, k+1)\n",
    "    plt.imshow(el[0])\n",
    "    attrs = ''\n",
    "    for i, attr in enumerate(el[1]):\n",
    "        if attr:\n",
    "            attr_i = data['CelebA'].attr_names[i]\n",
    "            attrs += f'{attr_i}\\n'\n",
    "    plt.title(attrs)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "6ef8134b-f811-451f-863a-7d70f5eae813",
  "notebookPath": "Sem1 - CV/sem1_cv_student.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
