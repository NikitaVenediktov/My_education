{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "cr0woira8zqikxcyder5jd"
   },
   "source": [
    "# **Seminar 2 - Natural Language Processing**\n",
    "*Naumov Anton (Any0019)*\n",
    "\n",
    "*To contact me in telegram: @any0019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "65hugpsi2onakx0nvflwhc"
   },
   "source": [
    "## 1. Понимание рекуррентных архитектур"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7t345mjknsp0w0xpj2priek"
   },
   "source": [
    "### 1.1 Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "mc97wqb4r4952t2ouk2oh",
    "execution_id": "bbdf12af-e759-4986-a1df-04a58716832c"
   },
   "source": [
    "<img src=https://research.aimultiple.com/wp-content/uploads/2021/08/rnn-text.gif alt=\"RNN\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "yi200i1i69svpz4wf5g6w8"
   },
   "source": [
    "$$ H^{(t)} = \\tanh \\Big( W^{h x} \\cdot X^{(t)} + W^{h h} \\cdot H^{(t-1)} + b_{h} \\Big) $$\n",
    "$$ Y^{(t)} = W^{y h} \\cdot H^{(t)} + b_{y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2cygqkk0l0gib5ijfe7sy"
   },
   "source": [
    "### 1.2 Long-Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "qwruk4difyc34ay7mfi0z"
   },
   "source": [
    "<img src=https://cdn-images-1.medium.com/max/950/1*76Mikf2yo1Q3U76hp17v9Q.gif alt=\"LSTM\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "tn8178x5a5pkn6iyh6hlp",
    "execution_id": "b3b273cc-ba85-448a-ab0a-d731c9bbd67a"
   },
   "source": [
    "$$ \\text{input:} \\quad i^{(t)} = \\sigma \\Big( W^{i x} \\cdot X^{(t)} + W^{i h} \\cdot H^{(t-1)} + b_{i} \\Big) $$\n",
    "$$ \\text{output:} \\quad o^{(t)} = \\sigma \\Big( W^{h x} \\cdot X^{(t)} + W^{o h} \\cdot H^{(t-1)} + b_{o} \\Big) $$\n",
    "$$ \\text{forget:} \\quad f^{(t)} = \\sigma \\Big( W^{h x} \\cdot X^{(t)} + W^{f h} \\cdot H^{(t-1)} + b_{f} \\Big) $$\n",
    "$$ \\tilde{H}^{(t)} = \\tanh \\Big( W^{g x} \\cdot X^{(t)} + W^{g h} \\cdot H^{(t-1)} + b_{g} \\Big) $$\n",
    "$$ C^{(t)} = \\tilde{H}^{(t)} \\odot i^{(t)} + C^{(t-1)} \\odot f^{(t)} $$\n",
    "$$ H^{(t)} = \\tanh \\Big( C^{(t)} \\Big) \\odot o^{(t)} $$\n",
    "$$ Y^{(t)} = W^{y h} \\cdot H^{(t)} + b_{y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "zzxe396nbjj4yl0dethzir"
   },
   "source": [
    "### 1.3 Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "590l1cccxiycqshg7ofbd8"
   },
   "source": [
    "<img src=https://vbystricky.github.io/images/2021-05/gru.svg alt=\"LSTM\" width=\"55%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ckagbb97wmlv55bgwm5tqh"
   },
   "source": [
    "$$ \\text{update:} \\quad z^{(t)} = \\sigma \\Big( W^{z x} \\cdot X^{(t)} + W^{z h} \\cdot H^{(t-1)} + b_{i} \\Big) $$\n",
    "$$ \\text{reset:} \\quad r^{(t)} = \\sigma \\Big( W^{r x} \\cdot X^{(t)} + W^{r h} \\cdot H^{(t-1)} + b_{r} \\Big) $$\n",
    "$$ \\tilde{H}^{(t)} = \\tanh \\Big(  W^{\\tilde{h} x} \\cdot X^{(t)} + W^{\\tilde{h} h} \\cdot \\big( r^{(t)} \\odot H^{(t-1)} \\big) + b_{\\tilde{h}} \\Big) $$\n",
    "$$ H^{(t)} = (1 - z^{(t)}) \\odot H^{(t - 1)} + z^{(t)} \\odot \\tilde{H}^{(t)} $$\n",
    "$$ Y^{(t)} = W^{y h} \\cdot H^{(t)} + b_{y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "0edm63z22i4j25sxc8um5v"
   },
   "source": [
    "### 1.4 Как получить предсказание? Какие бывают задачи?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "d4rrrn06sfvjct2qr36h5",
    "execution_id": "dc5ccbcd-4a5f-4ce8-b1b2-b50fe8601870"
   },
   "source": [
    "**Sequence to sequence (seq2seq):**\n",
    "\n",
    "<img src=https://vbystricky.github.io/images/2021-05/rnn_seq_to_seq.svg alt=\"tags\" width=\"55%\"/>\n",
    "\n",
    "К примеру, определение частей речи в предложении\n",
    "\n",
    "<img src=https://vbystricky.github.io/images/2021-05/rnn_seq_to_seq_v2.svg alt=\"seq2seq\" width=\"55%\"/>\n",
    "\n",
    "К примеру, автоматический перевод\n",
    "\n",
    "**Sequence to one:**\n",
    "\n",
    "<img src=https://vbystricky.github.io/images/2021-05/rnn_seq_to_one.svg alt=\"one class\" width=\"55%\"/>\n",
    "\n",
    "К примеру, предсказание оценки, соответствующей данному ревью фильма\n",
    "\n",
    "**One to sequence:**\n",
    "\n",
    "<img src=https://vbystricky.github.io/images/2021-05/rnn_one_to_seq.svg alt=\"sequence\" width=\"55%\"/>\n",
    "\n",
    "К примеру, составление текстового описания по чему-нибудь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "222exff2ukv7sl17hrvg4m"
   },
   "source": [
    "## 2. Задача классификации текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "1m8s7sgnz786dkdqlnd5yr"
   },
   "source": [
    "### 2.1 Скачиваем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "dwzeov49g8d3d16tbgce1p"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# For datasphere\n",
    "%pip install torchtext torchdata\n",
    "\n",
    "# For google collab\n",
    "# !pip install torchtext torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "lsiknzjighrv5hqkfb6uja"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# # Apparently there is a problem with this code in DataSphere...\n",
    "\n",
    "# from torchtext import datasets\n",
    "# from IPython.display import clear_output\n",
    "# import os\n",
    "# import time\n",
    "# from datetime import timedelta\n",
    "\n",
    "# download_dir = './datasets'\n",
    "\n",
    "# all_datasets = [\n",
    "#     'AG_NEWS',\n",
    "#     'DBpedia',\n",
    "#     'YelpReviewPolarity',\n",
    "#     'YelpReviewFull',\n",
    "#     'YahooAnswers',\n",
    "#     'AmazonReviewPolarity',\n",
    "#     'AmazonReviewFull'\n",
    "# ]\n",
    "\n",
    "# data = {'train': {}, 'test': {}}\n",
    "# download_time = dict()\n",
    "# for i, dataset_name in enumerate(all_datasets):\n",
    "#     clear_output(True)\n",
    "    \n",
    "#     start_time = time.monotonic()\n",
    "    \n",
    "#     print(f'{i+1}/{len(all_datasets)}: Downloading dataset {dataset_name}')\n",
    "    \n",
    "#     download_exec = f'data[\\'train\\'][dataset_name] = datasets.{dataset_name}(root=\\'{os.path.join(download_dir, dataset_name)}\\', split=\\'train\\')'\n",
    "#     download_val_exec = f'data[\\'test\\'][dataset_name] = datasets.{dataset_name}(root=\\'{os.path.join(download_dir, dataset_name)}\\', split=\\'test\\')'\n",
    "    \n",
    "#     # NEVER REPEAT THIS AT HOME\n",
    "#     exec(download_exec)\n",
    "#     exec(download_val_exec)\n",
    "#     download_time[dataset_name] = timedelta(seconds=time.monotonic() - start_time)\n",
    "    \n",
    "#     for split in ['train', 'test']:\n",
    "#         data[split][dataset_name] = [(l, t) for (l, t) in data[split][dataset_name]]\n",
    "    \n",
    "\n",
    "# clear_output(True)\n",
    "# print('Download time:')\n",
    "# for k, v in download_time.items():\n",
    "#     print('{: <20} ~ {}'.format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "fey0qj9y9agi8utv97w04l"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# If in DataSphere use this then\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_folder = \"./datasets/AG_NEWS/AG_NEWS\"\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(df_folder, \"train.csv\"), header=None)\n",
    "df_test = pd.read_csv(os.path.join(df_folder, \"test.csv\"), header=None)\n",
    "\n",
    "all_datasets = [\"AG_NEWS\"]\n",
    "\n",
    "data = {\n",
    "    \"train\": {\"AG_NEWS\": list(map(lambda el: (el[0], el[1] + \" \" + el[2]), df_train.values.tolist()))},\n",
    "    \"test\": {\"AG_NEWS\": list(map(lambda el: (el[0], el[1] + \" \" + el[2]), df_test.values.tolist()))},\n",
    "}\n",
    "\n",
    "del df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "du77oaij53g8w0vgpsmxai"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "data[\"test\"][\"AG_NEWS\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "dmp0onxd5pux7x75vinbyh"
   },
   "source": [
    "### 2.2 Посмотрим на данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "qk893ngoogox7ztiwctsd"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# For datasphere\n",
    "%pip install termcolor\n",
    "\n",
    "# For google collab\n",
    "# !pip install termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "cxrc4pwi6w4fijqz4ucgrt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from termcolor import colored\n",
    "\n",
    "show_each = 5\n",
    "num_classes = {}\n",
    "\n",
    "for dataset_name in data['train']:\n",
    "    num_classes[dataset_name] = len(set([label for (label, _) in data['train'][dataset_name]]))\n",
    "    \n",
    "    train_size = len(data['train'][dataset_name])\n",
    "    test_size = len(data['test'][dataset_name])\n",
    "    print(colored(dataset_name + f' ~ {train_size} train examples' +\n",
    "                  f' and {test_size} test examples' +\n",
    "                  f' with {num_classes[dataset_name]} classes:\\n',\n",
    "                  'red', attrs=['bold', 'underline']))\n",
    "    \n",
    "    for i in range(0, train_size, train_size//show_each):\n",
    "        label, text = data['train'][dataset_name][i]\n",
    "        print(f'  ' + colored(f'ind', attrs=['bold', 'underline']) +\n",
    "              f' ~ {i}:\\n  ' + colored(f'label', attrs=['bold', 'underline']) +\n",
    "              f' ~ {label}\\n  ' + colored(f'text', attrs=['bold', 'underline']) +\n",
    "              f' ~ {text}\\n')\n",
    "    print('\\n============\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "0zpwjvqnao6p1fhpmq6hbgr"
   },
   "source": [
    "### 2.3 Готовим данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ftp5g4mtr725q2dixgmwo",
    "execution_id": "4eb4c36a-3373-420d-be98-6376d8c8d348"
   },
   "source": [
    "Очень много разных подходов:\n",
    "\n",
    "- стоп-слова\n",
    "- токенизация\n",
    "- лемматизация\n",
    "- стемминг\n",
    "- обучаемые эмбеддинги\n",
    "- лингвистические модели\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "1t5yxtgtl74zq79kejcso"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "        \n",
    "splits = ['train', 'test']\n",
    "\n",
    "# Creating tokenizer for english text\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "print(tokenizer(\"Some random text, another random text.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "m7fbtvytzqk4ka3lgvl1e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "max_size = [0] * (len(all_datasets) * len(splits))\n",
    "\n",
    "# Building vocabs\n",
    "def yield_tokens(it_list):\n",
    "    global max_size\n",
    "    for i, it in enumerate(tqdm(it_list)):\n",
    "        for _, text in it:\n",
    "            tokens = tokenizer(text)\n",
    "            max_size[i] = max(max_size[i], len(tokens))\n",
    "            yield tokens\n",
    "\n",
    "\n",
    "def flat_list(list_):\n",
    "    res = []\n",
    "    for el in list_:\n",
    "        res.extend(el)\n",
    "    return res\n",
    "\n",
    "\n",
    "vocab = {}\n",
    "print('Building vocabs:')\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(\n",
    "        flat_list(\n",
    "            [\n",
    "                [data[split][dataset_name] for dataset_name in all_datasets]\n",
    "                for split\n",
    "                in splits\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    specials=[\"<unk>\", \"<pad>\"]\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "8su0kxz8ypk7a3ppbh3ou5"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\"w@rd\" in vocab, \"text\" in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "6n3he7qdru73uqb551l40x"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "np9zi0lq3ib7g0kgl26piv"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "80izyc1svru2f4mgc2obvn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "unk_id = vocab['<unk>']\n",
    "pad_id = vocab['<pad>']\n",
    "print(f'<unk> ~ {unk_id}\\n<pad> ~ {pad_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ggv282gad12rmy9suivkt"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "text = 'Some random text, that I want to tokenize <pad> <pad> <pad> and one unrecognizable w@rd'\n",
    "print(f'tokenized:\\n---- {tokenizer(text)}\\n')\n",
    "print(f'indexes from vocab:\\n---- {vocab(tokenizer(text))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0yiohx3s835oqfv0vjn85"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import numpy as np\n",
    "\n",
    "max_size = np.array(max_size).reshape(2, -1)\n",
    "for i, dataset_name in enumerate(all_datasets):\n",
    "    print(f'{dataset_name} ~ max_size: {max_size[:,i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "enhie486q8mkog7dhn86wh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "chosen_dataset = 'AG_NEWS'\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "for i, dataset_name in enumerate(all_datasets):\n",
    "    if dataset_name == chosen_dataset:\n",
    "        max_ = max(max_size[:,i])\n",
    "print(f'maximal text length ~ {max_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "1ucnzsh1bwhi7761p2xjxi7"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torch\n",
    "\n",
    "# Text and label preprocessing\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "\n",
    "# Applying preprocessings with offsets to batch\n",
    "def collate_batch_offsets(batch):\n",
    "    \"\"\"\n",
    "    input: batch --> Iteratable (label, text)\n",
    "    output:\n",
    "        - label_list --> tensor<int> ~ len=bs, all labels from batch\n",
    "        - text_list --> tensor<int>, all texts tokenized and merged in one list\n",
    "        - offsets --> tensor<int> ~ len=bs+1, where does each individual text starts\n",
    "    \n",
    "    hint - tensor.cumsum(dim=?) --> cumulative sum across dim\n",
    "    \"\"\"\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # !!!<TODO YOUR CODE HERE>!!!\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    return label_list, text_list, offsets\n",
    "\n",
    "\n",
    "# Applying preprocessings with padding to batch\n",
    "def collate_batch_padding(batch):\n",
    "    \"\"\"\n",
    "    input: batch --> Iteratable (label, text)\n",
    "    output:\n",
    "        - label_list --> tensor<int> ~ len=bs, all labels from batch\n",
    "        - text_list --> tensor<int> ~ len=(bs x seq_len), all texts tokenized and merged in matrix with paddings\n",
    "    \n",
    "    hint - use `max_` value (or may be calculate local max_seq_len)\n",
    "    \"\"\"\n",
    "    label_list, text_list = [], []\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # !!!<TODO YOUR CODE HERE>!!!\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    return label_list, text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "yisv5ehe2ncngpj1gduzu"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Making dataloaders\n",
    "dataloaders_off = {split: {} for split in splits}\n",
    "dataloaders_pad = {split: {} for split in splits}\n",
    "\n",
    "print('Making dataloaders...')\n",
    "\n",
    "\n",
    "# # For all downloaded datasets\n",
    "# for split in splits:\n",
    "#     print(f'---- for {split}')\n",
    "#     for dataset_name in tqdm(all_datasets):\n",
    "#         dataloaders_off[split][dataset_name] = DataLoader(\n",
    "#             data[split][dataset_name],\n",
    "#             batch_size=16,\n",
    "#             shuffle=(split=='train'),\n",
    "#             drop_last=(split=='train'),\n",
    "#             collate_fn=collate_batch_offsets\n",
    "#         )\n",
    "#         dataloaders_pad[split][dataset_name] = DataLoader(\n",
    "#             data[split][dataset_name],\n",
    "#             batch_size=16,\n",
    "#             shuffle=(split=='train'),\n",
    "#             drop_last=(split=='train'),\n",
    "#             collate_fn=collate_batch_padding\n",
    "#         )\n",
    "\n",
    "\n",
    "# Quicker, but only for your chosen dataset\n",
    "for split in splits:\n",
    "    dataloaders_off[split][chosen_dataset] = DataLoader(\n",
    "        data[split][chosen_dataset],\n",
    "        batch_size=16,\n",
    "        shuffle=(split=='train'),\n",
    "        drop_last=(split=='train'),\n",
    "        collate_fn=collate_batch_offsets,\n",
    "    )\n",
    "    dataloaders_pad[split][chosen_dataset] = DataLoader(\n",
    "        data[split][chosen_dataset],\n",
    "        batch_size=16,\n",
    "        shuffle=(split=='train'),\n",
    "        drop_last=(split=='train'),\n",
    "        collate_fn=collate_batch_padding,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Done!\")\n",
    "    \n",
    "# Freeing space for efficiency\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "zak4l5ba7w8codgxjau6w",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "l_, t_, o_ = next(iter(dataloaders_off['train'][chosen_dataset]))\n",
    "print(f'labels ~ {l_.shape}\\n{l_}\\n')\n",
    "print(f'texts ~ {t_.shape}\\n{t_}\\n')\n",
    "print(f'offsets ~ {o_.shape}\\n{o_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "mqowayxmspvtzuk79wyp"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "l_, t_ = next(iter(dataloaders_pad['train'][chosen_dataset]))\n",
    "print(f'labels ~ {l_.shape}\\n{l_}\\n')\n",
    "print(f'texts ~ {t_.shape}\\n{t_}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "4i04m65qdkumrgugy0r2mb",
    "execution_id": "644698c5-6557-486b-b7ae-63d4053b75b1"
   },
   "source": [
    "### 2.4. GloVe эмбеддинги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "fjvzh67wzxv6gx3aoqv6ww",
    "execution_id": "8edcf1e4-7ec6-429f-acd3-ac0efaf6e1f6"
   },
   "source": [
    "Давайте скачаем предобученные glove вектора и инициализируем nn.Embedding ими, там где мы их знаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "f9n3xsicy5c9wf4z58pft7"
   },
   "outputs": [],
   "source": [
    "#!g1.1:bash\n",
    "wget -O glove.zip https://huggingface.co/stanfordnlp/glove/resolve/main/glove.840B.300d.zip\n",
    "\n",
    "# mirror https://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "kc4yh5ixd1c7hq1zghf6de"
   },
   "outputs": [],
   "source": [
    "#!g1.1:bash\n",
    "ls -sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "3dy67s8kyrlyebrt769e2"
   },
   "outputs": [],
   "source": [
    "#!g1.1:bash\n",
    "unzip glove.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "y8j6gcr6v2q1jb7cr7wlh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from tqdm import tqdm as tqdm_\n",
    "\n",
    "np.random.seed(19)\n",
    "\n",
    "# Открываем glove\n",
    "def load_glove_weights(file_path, vocab):\n",
    "    print(\"Loading Glove Weights\")\n",
    "    # Инициализируем веса для всех слов стандартным нормальным распределением\n",
    "    glove_weights = np.random.uniform(0, 1, (len(vocab), 300))\n",
    "    mask_found = np.zeros(len(vocab), dtype=bool)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in tqdm_(f, total=2196018):\n",
    "            line = line.split()\n",
    "            token = ' '.join(line[:-300])\n",
    "            embed = line[-300:]\n",
    "\n",
    "            if token in vocab:\n",
    "                # <TODO YOUR CODE HERE>\n",
    "                ...\n",
    "\n",
    "    print(f\"{mask_found.sum()} words from vocab of size {len(vocab)} loaded!\")\n",
    "\n",
    "    glove_weights[vocab['<pad>']] = np.zeros(300, dtype=np.float)\n",
    "    return glove_weights, mask_found\n",
    "\n",
    "\n",
    "glove_weights, mask_found = load_glove_weights('glove.840B.300d.txt', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "m0t7op29hfbqd4sc598wn"
   },
   "source": [
    "## 3. Составляем модель и пайплайн обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "smer2qcl8il2srxtxw1d8"
   },
   "source": [
    "### 3.1 Рекуррентная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "1vcoi3jejz1tqerm6532v"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from torch import nn\n",
    "from typing import Any\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# !!!<TODO YOUR CODE IN THIS BLOCK>!!!\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "class RecurrentModel(nn.Module):\n",
    "    \"\"\"\n",
    "        Network with recurrent block\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        rnn_type: Any,\n",
    "        num_layers: int,\n",
    "        bidirectional: bool,\n",
    "        dropout_p: float,\n",
    "        use_glove: bool,\n",
    "        freeze_glove: bool,\n",
    "        reduce: str = 'last',\n",
    "    ):\n",
    "        super(RecurrentModel, self).__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        D = 2 if bidirectional else 1\n",
    "        self.reduce = reduce\n",
    "        \n",
    "        rnn_types = [nn.RNN, nn.LSTM, nn.GRU]\n",
    "        if rnn_type not in rnn_types:\n",
    "            raise ValueError(f'rnn_cell should be one of {rnn_types}')\n",
    "        \n",
    "        reduce_ways = ['last', 'max', 'mean', 'sum']\n",
    "        assert reduce in reduce_ways, print(f'reduce must be one of {reduce_ways}')\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=input_size)\n",
    "        if use_glove is True:\n",
    "            assert input_size == 300, (\n",
    "                \"loaded GloVe embeddings have size 300, change input_size to 300 or do not use GloVe\"\n",
    "            )\n",
    "            self.embed.weight = nn.Parameter(\n",
    "                torch.from_numpy(glove_weights),\n",
    "                requires_grad = (not freeze_glove),\n",
    "            )\n",
    "        \n",
    "        self.rnn_layers = rnn_type(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        # <TODO YOUR CODE HERE>\n",
    "        self.to_class = nn.Linear(...)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, texts):\n",
    "        # <TODO YOUR CODE HERE>\n",
    "        embeds = self.embed(...)\n",
    "        \n",
    "        if self.rnn_type == nn.LSTM:\n",
    "            output, (hn, cn) = self.rnn_layers(embeds)\n",
    "        else:\n",
    "            output, hn = self.rnn_layers(embeds)\n",
    "        \n",
    "        if self.reduce == 'last':\n",
    "            output = ... # <TODO YOUR CODE HERE>\n",
    "        elif self.reduce == 'mean':\n",
    "            output = ... # <TODO YOUR CODE HERE>\n",
    "        elif self.reduce == 'max':\n",
    "            output = ... # <TODO YOUR CODE HERE>\n",
    "        elif self.reduce == 'sum':\n",
    "            output = ... # <TODO YOUR CODE HERE>\n",
    "        \n",
    "        return self.logsoftmax(self.to_class(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "zoiba60mdrsbuoqzr3os2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "hashing_trick_size = None\n",
    "\n",
    "if hashing_trick_size == None:\n",
    "    hashing_trick_size = len(vocab)\n",
    "\n",
    "model_rnn = RecurrentModel(\n",
    "    vocab_size = hashing_trick_size,\n",
    "    input_size = 300,\n",
    "    hidden_size = 100,\n",
    "    output_size = num_classes[chosen_dataset],\n",
    "    rnn_type = nn.RNN,\n",
    "    num_layers = 3,\n",
    "    bidirectional = True,\n",
    "    dropout_p = 0.3,\n",
    "    reduce = 'mean',\n",
    "    use_glove = True,\n",
    "    freeze_glove = True,\n",
    ").float()\n",
    "\n",
    "print('Model:', model_rnn, sep='\\n')\n",
    "\n",
    "labels_batch, texts_batch = next(iter(dataloaders_pad['train'][chosen_dataset]))\n",
    "\n",
    "print(f'\\nInput shape: {texts_batch.shape}')\n",
    "out = model_rnn(texts_batch % hashing_trick_size)\n",
    "print(f'Output shape: {out.shape}')\n",
    "\n",
    "print(f'\\nChecking that returned probabilities (all sums must be close to 1)',\n",
    "      out.exp().sum(-1).detach().numpy(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "q4eukdc1c2moaazvxqo8i"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Counting how many parameters does our model have\n",
    "def model_num_params(model):\n",
    "    sum_params = 0\n",
    "    sum_learnable_params = 0\n",
    "    for param in model.named_parameters():\n",
    "        num_params = np.prod(param[1].shape)\n",
    "        print(\n",
    "            '{: <31} ~  {: <8} params ~ grad: {}'.format(\n",
    "                param[0],\n",
    "                num_params,\n",
    "                param[1].requires_grad,\n",
    "            )\n",
    "        )\n",
    "        sum_params += num_params\n",
    "        if param[1].requires_grad:\n",
    "            sum_learnable_params += num_params\n",
    "    print(\n",
    "        f'\\nIn total:\\n  - {sum_params} params\\n  - {sum_learnable_params} learnable params'\n",
    "    )\n",
    "    return sum_params\n",
    "\n",
    "sum_params = model_num_params(model_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0qex2fh8v1gtq1ps267wcm"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "del model_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "y9bhvr8zgbicteobk7kf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_lstm = RecurrentModel(\n",
    "    vocab_size = hashing_trick_size,\n",
    "    input_size = 300,\n",
    "    hidden_size = 100,\n",
    "    output_size = num_classes[chosen_dataset],\n",
    "    rnn_type = nn.LSTM,\n",
    "    num_layers = 3,\n",
    "    bidirectional = False,\n",
    "    dropout_p = 0.3,\n",
    "    reduce = 'last',\n",
    "    use_glove = True,\n",
    "    freeze_glove = True,\n",
    ").float()\n",
    "\n",
    "print('Model:', model_lstm, sep='\\n')\n",
    "\n",
    "labels_batch, texts_batch = next(iter(dataloaders_pad['train'][chosen_dataset]))\n",
    "\n",
    "print(f'\\nInput shape: {texts_batch.shape}')\n",
    "out = model_lstm(texts_batch % hashing_trick_size)\n",
    "print(f'Output shape: {out.shape}')\n",
    "\n",
    "print(f'\\nChecking that returned probabilities (all sums must be close to 1)',\n",
    "      out.exp().sum(-1).detach().numpy(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "rn4upr4x64k459kjwo7dhn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "sum_params = model_num_params(model_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "zrxiy1r6qju8yi96xzqhl"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "del model_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "e3tnv1hmdauc97z93yr4pa"
   },
   "source": [
    "### 3.2 Оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "2wo852vblq6kt52w2fyti"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def create_model_and_optimizer(model_class, model_params, lr=1e-3, beta1=0.9, beta2=0.999, device=device):\n",
    "    model = model_class(**model_params).float()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr, [beta1, beta2])\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "b8og48h2fe9kin5xu4n7li"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "hashing_trick_size = None\n",
    "\n",
    "model_params = {\n",
    "    'vocab_size': hashing_trick_size if hashing_trick_size is not None else len(vocab),\n",
    "    'input_size': 300,\n",
    "    'hidden_size': 100,\n",
    "    'output_size': num_classes[chosen_dataset],\n",
    "    'rnn_type': nn.RNN,\n",
    "    'num_layers': 3,\n",
    "    'bidirectional': True,\n",
    "    'dropout_p': 0.3,\n",
    "    'reduce': 'mean',\n",
    "    'use_glove': True,\n",
    "    'freeze_glove': True,\n",
    "}\n",
    "\n",
    "model, optimizer = create_model_and_optimizer(\n",
    "    model_class = RecurrentModel, \n",
    "    model_params = model_params,\n",
    "    lr = 1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vxowvzafngmhcn2sxy2ea",
    "execution_id": "3da4765d-7188-4f0f-bc69-b0947aa4592c"
   },
   "source": [
    "### 3.3 Обучение/валидация одной эпохи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "hniud7z2vh5k4aqozue3xh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# !!!<TODO YOUR CODE IN THIS BLOCK>!!!\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# here as an example\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def train(model, optimizer, loader, criterion, ht=None):\n",
    "    model.train()\n",
    "    losses_tr = []\n",
    "    for labels, texts in tqdm(loader):\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # !!!<TODO YOUR CODE HERE>!!!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    return model, optimizer, np.mean(losses_tr)\n",
    "\n",
    "def val(model, loader, criterion, ht=None):\n",
    "    model.eval()\n",
    "    losses_val = []\n",
    "    with torch.no_grad():\n",
    "        for labels, texts in tqdm(loader):\n",
    "            # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            # !!!<TODO YOUR CODE HERE>!!!\n",
    "            # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    return np.mean(losses_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "dx4pvmfciqgbbw0wb2dffq",
    "execution_id": "fd5c2919-66ce-44c8-b12c-dc59a81202c5"
   },
   "source": [
    "### 3.4 Цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ub1juqtda9s0v9m86pfa84l"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import trange\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "\n",
    "def learning_loop(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    scheduler=None,\n",
    "    min_lr=None,\n",
    "    epochs=10,\n",
    "    val_every=1,\n",
    "    draw_every=1,\n",
    "    separate_show=False,\n",
    "    hashing_trick_size=None,\n",
    "):\n",
    "    losses = {'train': [], 'val': []}\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f'#{epoch}/{epochs}:')\n",
    "        model, optimizer, loss = train(\n",
    "            model,\n",
    "            optimizer,\n",
    "            train_loader,\n",
    "            criterion,\n",
    "            hashing_trick_size,\n",
    "        )\n",
    "        losses['train'].append(loss)\n",
    "\n",
    "        if not (epoch % val_every):\n",
    "            loss = val(model, val_loader, criterion, hashing_trick_size)\n",
    "            losses['val'].append(loss)\n",
    "            if scheduler:\n",
    "                scheduler.step(loss)\n",
    "\n",
    "        if not (epoch % draw_every):\n",
    "            clear_output(True)\n",
    "            fig, ax = plt.subplots(1, 2 if separate_show else 1, figsize=(20, 10))\n",
    "            fig.suptitle(f'#{epoch}/{epochs}:')\n",
    "\n",
    "            if separate_show:\n",
    "                plt.subplot(121)\n",
    "                plt.title('loss on train')\n",
    "            plt.plot(losses['train'], 'r.-', label='train')\n",
    "            plt.legend()\n",
    "\n",
    "            if separate_show:\n",
    "                plt.subplot(122)\n",
    "                plt.title('loss on validation')\n",
    "            else:\n",
    "                plt.title('losses')\n",
    "            plt.plot(losses['val'], 'g.-', label='val')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.show()\n",
    "        \n",
    "        if min_lr and get_lr(optimizer) <= min_lr:\n",
    "            print(f'Learning process ended with early stop after epoch {epoch}')\n",
    "            break\n",
    "    \n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "5famw0ztk3jt3kxck585s9",
    "execution_id": "8a9947c8-c57b-4ee1-b2f9-61506837bc4b"
   },
   "source": [
    "### 3.5 Обучаем модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "xg7g7g8aqjq8v8taotsozj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_params = dict()\n",
    "model = dict()\n",
    "optimizer = dict()\n",
    "scheduler = dict()\n",
    "criterion = dict()\n",
    "losses = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "2axk8c12vcbm6po8c61n2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%%time\n",
    "\n",
    "model_type = 'bi-rnn'\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "hashing_trick_size = None\n",
    "\n",
    "model_params[model_type] = {\n",
    "    'vocab_size': hashing_trick_size if hashing_trick_size is not None else len(vocab),\n",
    "    'input_size': 300,\n",
    "    'hidden_size': 20,\n",
    "    'output_size': num_classes[chosen_dataset],\n",
    "    'rnn_type': nn.RNN,\n",
    "    'num_layers': 2,\n",
    "    'bidirectional': True,\n",
    "    'dropout_p': 0.3,\n",
    "    'reduce': 'mean',\n",
    "    'use_glove': True,\n",
    "    'freeze_glove': True,\n",
    "}\n",
    "\n",
    "model[model_type], optimizer[model_type] = create_model_and_optimizer(\n",
    "    model_class = RecurrentModel, \n",
    "    model_params = model_params[model_type],\n",
    "    lr = 5e-4,\n",
    "    device = device,\n",
    ")\n",
    "\n",
    "scheduler[model_type] = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer[model_type],\n",
    "    mode='min',\n",
    "    factor=0.25,\n",
    "    patience=4,\n",
    "    threshold=0.001,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "criterion[model_type] = nn.NLLLoss()\n",
    "\n",
    "model[model_type], optimizer[model_type], losses[model_type] = learning_loop(\n",
    "    model = model[model_type],\n",
    "    optimizer = optimizer[model_type],\n",
    "    train_loader = dataloaders_pad['train'][chosen_dataset],\n",
    "    val_loader = dataloaders_pad['test'][chosen_dataset],\n",
    "    criterion = criterion[model_type],\n",
    "    scheduler = scheduler[model_type],\n",
    "    epochs = 200,\n",
    "    min_lr = 2e-6,\n",
    "    hashing_trick_size = hashing_trick_size,\n",
    ")\n",
    "\n",
    "# if device != 'cpu':\n",
    "#     model[model_type] = model[model_type].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "azugm9jitkg8sd5ianivo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# %whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "n9lqyb7z63k3swod3aydt8",
    "execution_id": "b7674e51-56e6-43f2-ae53-b8c3fc448d08"
   },
   "source": [
    "### 3.6 Протестируем что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "mswtxmuuscqpdd4t3356e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_type = 'bi-rnn'\n",
    "\n",
    "# for AG_NEWS for example ~ search for others labels if needed\n",
    "true_labels = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "def predict(text):\n",
    "    processed_text = text_pipeline(text)\n",
    "    processed_text = [processed_text + [pad_id] * (max_ - len(processed_text))]\n",
    "    text_tensor = torch.tensor(processed_text, dtype=torch.int64)\n",
    "    if hashing_trick_size is not None:\n",
    "        text_tensor = text_tensor % hashing_trick_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs = model[model_type](text_tensor.to(device)).exp().detach().cpu().numpy()[0]\n",
    "        print(f\"Text:\\n    '{text}'\\n\")\n",
    "        for p, l in zip(probs, true_labels):\n",
    "            print(f\"{l} ~ {p}\")\n",
    "        \n",
    "    return [(p, l) for p, l in zip(probs, true_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "2cw2erhcbu7t2li12b43eo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "text = \"\"\"Apple will release it's new Iphone this weekend! What a great news!\"\"\"\n",
    "\n",
    "_ = predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "44mjynk0q7gwpfiyq03kad"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "text = \"\"\"Today, at grand arena, lions ones again claimed victory over tigers and gained the title of the world champions!\"\"\"\n",
    "\n",
    "_ = predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "y77gpythujauriblpf0aaa"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def real_confusion_matrix(model, val_loader, class_labels, use_probs=False, normalize=True):\n",
    "    with torch.no_grad():\n",
    "        n_classes = len(class_labels)\n",
    "        conf_matrix = np.zeros((n_classes, n_classes))\n",
    "        for i, (labels, texts) in enumerate(tqdm(val_loader)):\n",
    "            probs = model(texts.to(device)).exp()\n",
    "            if use_probs:\n",
    "                for j in range(texts.shape[0]):\n",
    "                    for c in range(n_classes):\n",
    "                        conf_matrix[labels[j].item(), c] += probs[j,c]\n",
    "            else:\n",
    "                _, pred_classes = torch.max(probs, 1)\n",
    "                for j in range(texts.shape[0]):\n",
    "                    conf_matrix[labels[j].item(), pred_classes[j].item()] += 1.\n",
    "        \n",
    "        if normalize:\n",
    "            conf_matrix /= conf_matrix.sum(1)\n",
    "        \n",
    "        fig = plt.figure(figsize=(18, 10))\n",
    "        fig.suptitle(f'Confusion matrix (norm={normalize}, use_probs={use_probs})')\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(conf_matrix)\n",
    "        fig.colorbar(cax)\n",
    "        \n",
    "        @plt.FuncFormatter\n",
    "        def fake_labels(x, pos):\n",
    "            return class_labels[(int(x))] if x < len(class_labels) else \"@\"\n",
    "        \n",
    "        ax.xaxis.set_major_formatter(fake_labels)\n",
    "        ax.set_xlabel('predicted class')\n",
    "#         ax.xaxis.set_major_formatter(\"\")\n",
    "#         secax = ax.secondary_xaxis('top')\n",
    "#         secax.xaxis.set_ticks(list(range(len(class_labels))))\n",
    "#         secax.xaxis.set_ticklabels(class_labels)\n",
    "#         secax.set_xlabel('predicted class')\n",
    "        \n",
    "        ax.yaxis.set_ticks(list(range(len(class_labels))))\n",
    "        ax.yaxis.set_ticklabels(class_labels)\n",
    "        ax.set_ylabel('true class')\n",
    "        \n",
    "        \n",
    "        for x in range(conf_matrix.shape[0]):\n",
    "            for y in range(conf_matrix.shape[1]):\n",
    "                ax.text(x, y, round(conf_matrix[x,y], 4), va='center', ha='center')\n",
    "        \n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "pjae2lxfa5vjj1jolfevp"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "pcm = real_confusion_matrix(\n",
    "    model[model_type],\n",
    "    dataloaders_pad[\"test\"][chosen_dataset],\n",
    "    true_labels,\n",
    "    use_probs=True,\n",
    "    normalize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "xgz2oqa688fsd2ct0ovc8f"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "pcm = real_confusion_matrix(\n",
    "    model[model_type],\n",
    "    dataloaders_pad[\"test\"][chosen_dataset],\n",
    "    true_labels,\n",
    "    use_probs=False,\n",
    "    normalize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "v540o4tsnoe4b5amxrejbc",
    "execution_id": "8a6fac40-6947-4ba8-aaaf-a30a220d6b65"
   },
   "source": [
    "## 4. Эмбеддинги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "njj8olbmsh2zip8t2698n"
   },
   "source": [
    "### 4.1 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "d80ux4ieczaq65gjry9gpn",
    "execution_id": "79f3897d-a3c5-4188-a48a-be8006f5096e"
   },
   "source": [
    "Идея алгоритма простая: обучим не рекуррентную нейросеть на нахождение связи между словом и контекстом, в котором оно встречается в корпусе.\n",
    "\n",
    "Все тексты токенезируются, затем берётся окно фиксированного размера и для кажджого текста проводят обучение на каждой из возможных позиций центра и окна. Далее, внутреннее представление модели для данного слова в центре - и есть его эмбеддинг.\n",
    "\n",
    "Есть два алгоритма, относящиеся к Word2Vec:\n",
    "\n",
    "1. **CBow** (Continuous Bag of Words) - предсказания слова по контексту\n",
    "\n",
    "<img src=\"https://amitness.com/images/nlp-ssl-center-word-prediction.gif\" alt=\"CBow\" width=\"90%\"/>\n",
    "\n",
    "\n",
    "2. **Skip-gram** - предсказание контекста по слову\n",
    "\n",
    "<img src=\"https://amitness.com/images/nlp-ssl-neighbor-word-prediction.gif\" alt=\"Skip-gram\" width=\"90%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "9ynozkfzcvj5g6042pzp96",
    "execution_id": "142bd094-3a90-4d02-9247-78686289369d"
   },
   "source": [
    "<img src=\"https://img-blog.csdnimg.cn/20190903155003926.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NoZW54aW52aGFpODk=,size_16,color_FFFFFF,t_70\" alt=\"word2vec\" width=\"95%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3s9n1527g583uft4junx2"
   },
   "source": [
    "**Плюсы:**\n",
    "- Похожие по контексту слова оказываются похожди в пространстве эмбеддингов\n",
    "- Смысловые зависимости между словами сохраняются при математических операциях\n",
    "<img src=\"https://habrastorage.org/r/w1560/getpro/habr/post_images/d61/30b/d45/d6130bd4502710631a8c812923230f47.png\" alt=\"word2vec\" width=\"80%\"/>\n",
    "<img src=\"https://amitness.com/images/word2vec-analogy.gif\" alt=\"word2vec\" width=\"80%\"/>\n",
    "<img src=\"https://habrastorage.org/r/w1560/getpro/habr/post_images/a19/84b/fea/a1984bfeab5a597c6fb6300f7d694901.png\" alt=\"word2vec\" width=\"80%\"/>\n",
    "\n",
    "**Минусы:**\n",
    "- Плохо справляется со словами вне словаря. К примеру, если в текстах корпуса встречались достаточно часто слова 'tensor' и 'flow', но не встречался 'tensorflow', то модель пометит последний как просто '\\<unk\\>'\n",
    "- Никак не переиспользует параметры для слов с общими корнями (к пр. 'eat', 'eats', 'eaten', 'eater', 'eating') - для модели это просто разные слова и каждое из них учится отдельно из контекста, тогда как слова сильно завязаны по смыслу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7xlg5p2zke3fsfsi5hoa1"
   },
   "source": [
    "### 4.2 GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "4qj21hh4tj4w9v8tjlc9z"
   },
   "source": [
    "**GloVe** (Global vectors) - нечто среднее между word2vec и SVD разложением.\n",
    "\n",
    "Модель GloVe пытается решить проблему эффективного использования статистики совпадений. GloVe минимизирует разницу между произведением векторов слов и логарифмом вероятности их совместного появления с помощью SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "i4ey8juw828v3svmxvy0p"
   },
   "source": [
    "### 4.3 FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "z1ilza0p3p3auhvgm3apn"
   },
   "source": [
    "Идея алгоритма следующая: будем обучать word2vec, но не над словами, а над буквенными n-gram-ами\n",
    "\n",
    "Берём каждое слово, добавляем к нему <...>, чтобы обозначать границы слова.\n",
    "\n",
    "Дальше бьём каждое слово на n-gram-мы фиксированного размера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "yo0qg3ra2excxmejg87x"
   },
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr><th>Слово</th><th>n-gram</th><th>Разбиение</th></tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>eating</td><td>3</td><td>&lt;ea, eat, ati, tin, ing, ng&gt;</td></tr>\n",
    "<tr><td>eating</td><td>4</td><td>&lt;eat, eati, atin, ting, ing&gt;</td></tr>\n",
    "<tr><td>eating</td><td>5</td><td>&lt;eati, eatin, ating, ting&gt;</td></tr>\n",
    "<tr><td>eating</td><td>6</td><td>&lt;eatin, eating, ating&gt;</td></tr>\n",
    "</tbody>\n",
    "<\\table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vy21gqa43ancz6t8aca1"
   },
   "source": [
    "Токенизируем (так же используем hashing trick, чтобы ограничить размер словаря).\n",
    "\n",
    "<img src=\"https://amitness.com/images/fasttext-hashing-ngrams.png\" alt=\"hashing_trick\" width=\"80%\"/>\n",
    "\n",
    "Используем суммы всех эмбедингов токенов и слова целиком, чтобы получить эмбеддинг слова.\n",
    "\n",
    "<img src=\"https://amitness.com/images/fasttext-center-word-embedding.png\" alt=\"word embeddings\" width=\"80%\"/>\n",
    "\n",
    "Обучаем на положительных и негативных примерах - негативные сэмплим случайно с вероятностью пропорциональной корню из вероятности слова. Для каждого положительного сэмплим 5 негативных.\n",
    "\n",
    "<img src=\"https://amitness.com/images/fasttext-negative-sampling-goal.png\" alt=\"negative sampling\" width=\"80%\"/>\n",
    "\n",
    "Обучаем при помощи SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "jindhk5q7skr1lmcm94ac"
   },
   "source": [
    "### 4.4 ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "llk4cwxofne45r4r5idcxi"
   },
   "source": [
    "**ELMo** (Embeddings from Language Model)\n",
    "\n",
    "Вместо того, чтобы использовать фиксированные эмбеддинги слов, ELMo смотрит на целое предложение, прежде чем присвоить каждому слову его эмбеддинг. Она использует двунаправленную модель долгой краткосрочной памяти (bi-directional LSTM), обученную специально под задачу создания таких эмбеддингов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "sut0kxu59gt806ajlxt2a"
   },
   "source": [
    "### 4.5 And then you'll need attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "pi2ojfxkofo4kxu5ik3m"
   },
   "source": [
    "ELMo - первый подход к лингивтическому моделированию\n",
    "\n",
    "Далее появляются архитектуры с механизмом внимания, но это уже совсем другая история...\n",
    "\n",
    "(to be continued на курсе NLP)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "2282daa9-a912-453f-b575-27c248d446c8",
  "notebookPath": "Sem2 - NLP/sem2_nlp_student_fix.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
