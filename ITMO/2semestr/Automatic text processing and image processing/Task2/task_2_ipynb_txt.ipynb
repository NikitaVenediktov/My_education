{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd_ST0GfO97y"
      },
      "source": [
        "# Информационный поиск\n",
        "\n",
        "Заменяем текст запроса по заданию"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d6Uze9jOV9EO"
      },
      "outputs": [],
      "source": [
        "QUERIES = ['flow behaviour.']   "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4etK3pClWELD"
      },
      "source": [
        "Скачиваем данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHflLH2APAHK",
        "outputId": "21a32ee1-66f7-4335-af5a-f97006ee3c68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\"wget\" �� ���� ����७��� ��� ���譥�\n",
            "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n",
            "tar: Error opening archive: Failed to open 'cran.tar.gz'\n",
            "\"rm\" �� ���� ����७��� ��� ���譥�\n",
            "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n",
            "\"grep\" �� ���� ����७��� ��� ���譥�\n",
            "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n",
            "\"head\" �� ���� ����७��� ��� ���譥�\n",
            "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n"
          ]
        }
      ],
      "source": [
        "!wget -q http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz\n",
        "!tar -xvf cran.tar.gz\n",
        "!rm cran.tar.gz*\n",
        "!grep -v \"^\\.\" cran.qry > just.qry\n",
        "!head -3 just.qry\n",
        "# !pip install -q scikit-learn==0.22.2.post1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zddS-EsnWP6U"
      },
      "source": [
        "Запускаем код"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBaV3xeQiUam",
        "outputId": "1ab3c81d-33f0-4dca-bf67-e81e7242c669"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "empty vocabulary; perhaps the documents only contain stop words",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\TurboFen\\Desktop\\Online_Edu\\My_education\\ITMO\\2semestr\\Automatic text processing and image processing\\Task2\\task_2_ipynb_txt.ipynb Cell 6\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TurboFen/Desktop/Online_Edu/My_education/ITMO/2semestr/Automatic%20text%20processing%20and%20image%20processing/Task2/task_2_ipynb_txt.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     query_data\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TurboFen/Desktop/Online_Edu/My_education/ITMO/2semestr/Automatic%20text%20processing%20and%20image%20processing/Task2/task_2_ipynb_txt.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m encoder \u001b[39m=\u001b[39m CountVectorizer(binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TurboFen/Desktop/Online_Edu/My_education/ITMO/2semestr/Automatic%20text%20processing%20and%20image%20processing/Task2/task_2_ipynb_txt.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m encoded_data \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mfit_transform(query_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TurboFen/Desktop/Online_Edu/My_education/ITMO/2semestr/Automatic%20text%20processing%20and%20image%20processing/Task2/task_2_ipynb_txt.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m encoded_queries \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39mtransform(QUERIES)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TurboFen/Desktop/Online_Edu/My_education/ITMO/2semestr/Automatic%20text%20processing%20and%20image%20processing/Task2/task_2_ipynb_txt.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m id2term \u001b[39m=\u001b[39m {idx: term \u001b[39mfor\u001b[39;00m term, idx \u001b[39min\u001b[39;00m encoder\u001b[39m.\u001b[39mvocabulary_\u001b[39m.\u001b[39mitems()}\n",
            "File \u001b[1;32mc:\\Users\\TurboFen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\TurboFen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1293\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1292\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1293\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1294\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m         )\n\u001b[0;32m   1297\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
            "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
          ]
        }
      ],
      "source": [
        "from  sklearn.feature_extraction.text import CountVectorizer\n",
        "from  sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import scipy.spatial.distance as ds \n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def cosine_distance(vector_a: np.array, vector_b: np.array) -> float:\n",
        "  return ds.cosine(vector_a, vector_b)\n",
        "\n",
        "def jaccard_sim(vector_a: np.array, vector_b: np.array) -> float:\n",
        "  vector_a = np.asarray(vector_a, np.bool)\n",
        "  vector_b = np.asarray(vector_b, np.bool)\n",
        "  return np.double(np.bitwise_and(vector_a, vector_b).sum()) / np.double(np.bitwise_or(vector_a, vector_b).sum())\n",
        "\n",
        "raw_query_data = [line.strip() for line in open(\"just.qry\", \"r\").readlines()]\n",
        "query_data = [\"\"]\n",
        "\n",
        "for query_part in raw_query_data:\n",
        "  query_data[-1] += query_part + \" \"\n",
        "  if query_part.endswith(\".\"):\n",
        "    query_data.append(\"\")\n",
        "\n",
        "encoder = CountVectorizer(binary=True)\n",
        "encoded_data = encoder.fit_transform(query_data)\n",
        "encoded_queries = encoder.transform(QUERIES)\n",
        "\n",
        "id2term = {idx: term for term, idx in encoder.vocabulary_.items()}\n",
        "non_zero_values_ids = encoded_data[0].nonzero()[1]\n",
        "\n",
        "terms = [id2term[idx] for idx in non_zero_values_ids]\n",
        "\n",
        "print(\"По мера Жаккара:\")\n",
        "for q_id, query in enumerate(encoded_queries):\n",
        "  query = query.todense().A1\n",
        "  docs = [doc.todense().A1 for doc in encoded_data]\n",
        "  id2doc2similarity = [(doc_id, doc, jaccard_sim(query, doc)) for doc_id, doc in enumerate(docs)]\n",
        "  closest = sorted(id2doc2similarity, key=lambda x: x[2], reverse=True)\n",
        "  \n",
        "  print(\"Q: %s:\" %(QUERIES[q_id]))\n",
        "  print(\"    %s\\t%s\" %(\"ID\", \"Коэффициент\"))\n",
        "  for closest_id, _, sim in closest[:2]:\n",
        "    print(\"    %d\\t%.2f\" %(closest_id, sim))\n",
        "\n",
        "print()\n",
        "\n",
        "# Второе задание\n",
        "tfidf_encoder = TfidfVectorizer()\n",
        "tfidf_encoded_data = tfidf_encoder.fit_transform(query_data)\n",
        "tfidf_encoded_queries = tfidf_encoder.transform(QUERIES)\n",
        "\n",
        "print(\"Косинустное расстояние:\")\n",
        "for q_id, query in enumerate(tfidf_encoded_queries):\n",
        "  query = query.todense().A1\n",
        "  docs = [doc.todense().A1 for doc in tfidf_encoded_data]\n",
        "  id2doc2similarity = [(doc_id, doc, cosine_distance(query, doc)) for doc_id, doc in enumerate(docs)]\n",
        "  closest = sorted(id2doc2similarity, key=lambda x: x[2], reverse=False)\n",
        "\n",
        "  print(\"Q: %s:\" %(QUERIES[q_id]))\n",
        "  print(\"    %s\\t%s\" %(\"ID\", \"Коэффициент\"))\n",
        "  for closest_id, _, sim in closest[:3]:\n",
        "    print(\"    %d\\t%.2f\" %(closest_id, sim))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
