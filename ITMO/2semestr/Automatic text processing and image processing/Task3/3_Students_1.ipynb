{"cells":[{"cell_type":"markdown","metadata":{"id":"TAyzWDzFdnLa"},"source":["## Классификация текстов: Spam or Ham"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["https://courses.openedu.ru/asset-v1:ITMOUniversity+AUTXTIMGPROC+spring_2023_ITMO_mag+type@asset+block/3_Students_1.html"]},{"cell_type":"markdown","metadata":{"id":"sh91cKPmdnLb"},"source":["В этом задании на примере классического датасета Spambase Dataset (https://archive.ics.uci.edu/ml/datasets/spambase) мы попробуем сделать свой спам-фильтр c помощью библиотеки scikit-learn. Датасет содержит корпус текстов 5,574 смс с метками \"spam\" и \"ham\". "]},{"cell_type":"markdown","metadata":{"id":"D4Bf-AmgdnLb"},"source":["### Данные"]},{"cell_type":"markdown","metadata":{"id":"lWvXDXKrdnLb"},"source":["Для удобства данные приложены в описании задания"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UnJwvQzbdnLb"},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv('data.csv', encoding='latin-1')"]},{"cell_type":"markdown","metadata":{"id":"GRqzedIIdnLc"},"source":["Оставляем только интересующие нас колонки — тексты смс и метки:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xO59-HEadnLc","outputId":"470c6b7d-6e6f-4105-eab4-947debd213ef"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ham</td>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ham</td>\n","      <td>Ok lar... Joking wif u oni...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>spam</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ham</td>\n","      <td>U dun say so early hor... U c already then say...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ham</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  label                                               text\n","0   ham  Go until jurong point, crazy.. Available only ...\n","1   ham                      Ok lar... Joking wif u oni...\n","2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n","3   ham  U dun say so early hor... U c already then say...\n","4   ham  Nah I don't think he goes to usf, he lives aro..."]},"execution_count":2,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["df = df[['v1', 'v2']]\n","df = df.rename(columns = {'v1': 'label', 'v2': 'text'})\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"NU2xwmvIdnLd"},"source":["Удаляем дублирующиеся тексты:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iveCG_bXdnLd"},"outputs":[],"source":["df = df.drop_duplicates('text')"]},{"cell_type":"markdown","metadata":{"id":"XuPip3mzdnLd"},"source":["Заменяем метки на бинарные:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsKdfy6-dnLd"},"outputs":[],"source":["df['label'] = df['label'].map({'ham': 0, 'spam': 1})"]},{"cell_type":"markdown","metadata":{"id":"8vQJK8LNdnLe"},"source":["### Предобработка текста (Задание 1)"]},{"cell_type":"markdown","metadata":{"id":"D8tefVdTdnLe"},"source":["Нужно дополнить функцию для предобработки сообщений, которая делает с текстом следующее:\n","* приводит текст к нижнему регистру;\n","* удаляет стоп-слова;\n","* удаляет знаки препинания;\n","* нормализует текст при помощи стеммера Snowball.\n","\n","Предлагаем воспользоваться библиотекой nltk, чтобы не составлять список стоп-слов и не реализовывать алгоритм стемминга самостоятельно. Примеры использования стеммеров можно найти по ссылке (https://www.nltk.org/howto/stem.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yhU1BvAHdnLe"},"outputs":[],"source":["from nltk import stem\n","from nltk.corpus import stopwords\n","import re\n","\n","stemmer = stem.SnowballStemmer('english')\n","stopwords = set(stopwords.words('english'))\n","\n","def preprocess(text):\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    # Ваш код здесь\n","    return text"]},{"cell_type":"markdown","metadata":{"id":"QM-Lrt6ddnLe"},"source":["Проверка, что функция работает верно"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RSuPqUb2dnLe"},"outputs":[],"source":["assert preprocess(\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\") == \"im gonna home soon dont want talk stuff anymor tonight k ive cri enough today\"\n","assert preprocess(\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\") == \"go jurong point crazi avail bugi n great world la e buffet cine got amor wat\""]},{"cell_type":"markdown","metadata":{"id":"84H8mOmpdnLe"},"source":["Применяем получившуюся функцию к текстам:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NtM4626ednLe"},"outputs":[],"source":["df['text'] = df['text'].apply(preprocess)"]},{"cell_type":"markdown","metadata":{"id":"gRyqtqUtdnLe"},"source":["### Разделение данных на обучающую и тестовую выборки (Задание 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nt7Z5NCMdnLe"},"outputs":[],"source":["y = df['label'].values"]},{"cell_type":"markdown","metadata":{"id":"6r4PJBctdnLe"},"source":["Теперь нужно разделить данные на тестовую (test) и обучающую (train) выборку. В библиотеке scikit-learn для этого есть готовые инструменты."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1c9ARWIdnLe"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.3, random_state=51)"]},{"cell_type":"markdown","metadata":{"id":"KOV7Ub4ldnLe"},"source":["### Обучение классификатора (Задание 3)"]},{"cell_type":"markdown","metadata":{"id":"enAzNefqdnLe"},"source":["Переходим к обучению классификатора.\n","\n","Сначала извлекаем признаки из текстов. Рекомендуем попробовать разные способы и посмотреть, как это влияет на результат (подробнее о различных способах представления текстов можно прочитать по ссылке https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction).\n","\n","Затем обучаем классификатор. Мы используем SVM, но можно поэкспериментировать с различными алгоритмами."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtfcmJ7NdnLe"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","\n","# извлекаем признаки из текстов\n","vectorizer = TfidfVectorizer(decode_error='ignore')\n","X_train = vectorizer.fit_transform(X_train)\n","X_test = vectorizer.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIQQo8j3dnLe"},"outputs":[],"source":["from sklearn.svm import LinearSVC\n","from sklearn.metrics import classification_report\n","\n","#обучаем подель SVM\n","\n","model = LinearSVC(random_state = 51, C = 1)\n","model.fit(X_train, y_train)\n","predictions = model.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"sn9kvQaZdnLe"},"source":["Для самопроверки. Если вы верно дополнили функцию ```preprocess```, то должны получиться следующие результаты оценки модели."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGxDEYnjdnLe","outputId":"104a68f1-cf8a-421e-cc3f-2b63b99c2520"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0      0.984     0.993     0.988      1355\n","           1      0.946     0.888     0.916       196\n","\n","    accuracy                          0.979      1551\n","   macro avg      0.965     0.940     0.952      1551\n","weighted avg      0.979     0.979     0.979      1551\n","\n"]}],"source":["print(classification_report(y_test, predictions, digits=3))"]},{"cell_type":"markdown","metadata":{"id":"1nffLu6UdnLf"},"source":["Выполним предсказание для конкретного текста"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"prWswDzudnLf"},"outputs":[],"source":["txt = \"As a valued customer, I am pleased to advise you that following recent review of your Mob No. you are awarded with a å£1500 Bonus Prize, call 09066364589\"\n","txt = preprocess(txt)\n","txt = vectorizer.transform([txt])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"brfpnzR7dnLf","outputId":"5220b6fe-a3a6-45f4-e726-b3f813ac7751"},"outputs":[{"data":{"text/plain":["array([1])"]},"execution_count":20,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["model.predict(txt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVfQFiwGdnLf"},"outputs":[],"source":["Сообщение помечено как спам."]}],"metadata":{"colab":{"collapsed_sections":[],"name":"3_Students.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
